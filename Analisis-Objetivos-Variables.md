# ANÃLISIS TÃ‰CNICO INTEGRAL: OBJETIVOS, VARIABLES Y DISEÃ‘O DE EXPERIMENTOS
## MediciÃ³n AutomÃ¡tica de Estados Afectivos en Aulas HÃ­bridas mediante Edge AI Multimodal

**Autor:** AnÃ¡lisis tÃ©cnico especializado  
**Fecha:** Diciembre 2025  
**Contexto:** MaestrÃ­a en Inteligencia Artificial - Proyecto de InvestigaciÃ³n  
**Universidad:** Universidad Nacional de IngenierÃ­a (UNI)

---

## 1. INTRODUCCIÃ“N Y MARCO CONCEPTUAL

### 1.1 Problema Central de InvestigaciÃ³n

En el contexto de la educaciÃ³n hÃ­brida moderna, los docentes enfrentan un desafÃ­o crÃ­tico: monitorear efectivamente el nivel de atenciÃ³n y compromiso de estudiantes que participan simultÃ¡neamente en modalidad presencial y remota. Las soluciones actuales basadas en computaciÃ³n en la nube presentan limitaciones significativas:

- **Latencia inaceptable** para retroalimentaciÃ³n en tiempo real
- **Riesgos de privacidad** al procesar datos biomÃ©tricos (anÃ¡lisis facial) en servidores externos
- **Alto consumo de recursos computacionales** e infraestructura costosa
- **Dependencia de conectividad** que limita su viabilidad en contextos con limitaciones tecnolÃ³gicas

### 1.2 SoluciÃ³n Propuesta: Edge AI Multimodal

Se propone una soluciÃ³n basada en **Edge AI** (Inteligencia Artificial en el Borde) que:

1. **Procesa datos localmente** en dispositivos de borde (Jetson Nano, Raspberry Pi 5)
2. **Integra anÃ¡lisis multimodal** (facial + postural) para mayor robustez
3. **Optimiza modelos ligeros** mediante cuantizaciÃ³n y poda para ejecuciÃ³n eficiente
4. **Proporciona alertas en tiempo real** (<100ms de latencia) sin dependencia de nube
5. **Preserva privacidad** al no transmitir datos biomÃ©tricos sensibles

### 1.3 Relevancia TeÃ³rica y PrÃ¡ctica

**Base TeÃ³rica:**
- **Aprendizaje Afectivo:** La atenciÃ³n y los estados emocionales impactan directamente en la retenciÃ³n y comprensiÃ³n [Hasnine et al., 2021; Wang et al., 2019]
- **Edge Computing:** Minimiza latencia, reduce consumo de datos y mejora privacidad [Abdulkader et al., 2023]
- **Procesamiento Multimodal:** Combinar anÃ¡lisis facial y postural proporciona detecciÃ³n mÃ¡s robusta que modalidades individuales [Hossen & Uddin, 2023; Pang et al., 2023]

**Relevancia Aplicada:**
- Herramienta inmediata para docentes en contexto de aulas hÃ­bridas
- RetroalimentaciÃ³n en tiempo real para adaptaciÃ³n pedagÃ³gica inmediata
- Cumplimiento normativo de privacidad de datos (Ley NÂ° 29733 PerÃº, GDPR)

---

## 2. REDEFINICIÃ“N DE OBJETIVOS (DE 4 A 5 OBJETIVOS)

### 2.1 Objetivo General

**DiseÃ±ar, implementar y validar un sistema Edge AI optimizado para la mediciÃ³n automÃ¡tica en tiempo real de estados afectivos de estudiantes universitarios en aulas hÃ­bridas, mediante anÃ¡lisis multimodal facial y postural, asegurando alta precisiÃ³n, baja latencia y utilidad pedagÃ³gica.**

### 2.2 Objetivos EspecÃ­ficos Redefinidos (5 Objetivos)

Los 4 objetivos originales se han desdobblado en 5 para mayor claridad y precisiÃ³n:

#### **OE1: AdquisiciÃ³n y NormalizaciÃ³n de Datos Afectivos Multimodales**
- **DescripciÃ³n:** Crear o seleccionar un dataset de video que capture estudiantes universitarios en aulas hÃ­bridas reales, etiquetar sus estados afectivos (AtenciÃ³n, DistracciÃ³n, Fatiga) mediante protocolo de anotaciÃ³n validado
- **Indicadores Clave:**
  - Tasa de muestras Ãºtiles: â‰¥95% de frames etiquetados correctamente
  - TamaÃ±o del dataset: nâ‰¥500 videos (mÃ­nimo 50 estudiantes diferentes)
  - Acuerdo inter-anotador (Cohen's Kappa): â‰¥0.85
  - Diversidad demogrÃ¡fica: â‰¥70% representatividad por gÃ©nero, etnia
- **TransformaciÃ³n/Output:** Dataset multimodal etiquetado, normalizado y dividido en train/val/test

#### **OE2: IdentificaciÃ³n y ExtracciÃ³n de CaracterÃ­sticas Faciales y Posturales**
- **DescripciÃ³n:** Identificar y extraer 20-30 caracterÃ­sticas clave (landmarks faciales, emociones, direcciÃ³n de mirada, pose corporal) que correlacionan directamente con estados de atenciÃ³n, distracciÃ³n y fatiga
- **Indicadores Clave:**
  - PrecisiÃ³n de extracciÃ³n de landmarks: â‰¥98% (validado contra ground truth manual)
  - NÃºmero de caracterÃ­sticas seleccionadas: 20-30 features
  - CorrelaciÃ³n con etiquetas de atenciÃ³n: râ‰¥0.75 (Pearson/Spearman)
  - Importancia de features (SHAP/permutation): Top 10 explica â‰¥70% de la varianza
- **TransformaciÃ³n/Output:** Conjunto de features ingenierÃ­a validado, matriz de importancia, anÃ¡lisis de correlaciÃ³n

#### **OE3: OptimizaciÃ³n y Despliegue de Modelos Ligeros en Arquitectura Edge**
- **DescripciÃ³n:** Seleccionar, entrenar y optimizar modelos ligeros (MobileNet, Mini-Xception, YOLO-Nano) usando tÃ©cnicas de cuantizaciÃ³n (8-bit) y poda (10-40%) para ejecutar en dispositivos Edge con recursos limitados
- **Indicadores Clave:**
  - PrecisiÃ³n del modelo: â‰¥90% (mantenida despuÃ©s de optimizaciÃ³n)
  - Latencia end-to-end: â‰¤100ms (desde captura de frame â†’ predicciÃ³n)
  - FPS: â‰¥25 frames por segundo
  - TamaÃ±o del modelo: â‰¤15MB (.tflite, .onnx)
  - Consumo de RAM: â‰¤250MB
  - Consumo de CPU: â‰¤40%
  - RetenciÃ³n de precisiÃ³n post-optimizaciÃ³n: â‰¥95%
- **TransformaciÃ³n/Output:** Modelos optimizados desplegables (.tflite, .onnx), benchmarks de performance, guÃ­as de despliegue

#### **OE4: DiseÃ±o e IntegraciÃ³n del Prototipo Funcional y Dashboard de Alertas**
- **DescripciÃ³n:** Desarrollar la lÃ³gica de fusiÃ³n multimodal (weighted average, voting ensemble), sistema de alertas contextualizadas en tiempo real, e interfaz de dashboard histÃ³rico para docentes
- **Indicadores Clave:**
  - Usabilidad percibida (Escala Likert 5 puntos): â‰¥4.0/5.0
  - PrecisiÃ³n de alertas: â‰¥80% (Precision mÃ©trica)
  - Tiempo de respuesta a evento de distracciÃ³n: â‰¤200ms
  - Cobertura de funcionalidades implementadas: â‰¥95% del spec
  - SatisfacciÃ³n de docentes: â‰¥4.0/5.0 (nâ‰¥20 evaluadores)
- **TransformaciÃ³n/Output:** Prototipo funcional, interfaz web/app, documentaciÃ³n de uso, manual tÃ©cnico

#### **OE5: ValidaciÃ³n Aplicada y EvaluaciÃ³n de Impacto PedagÃ³gico**
- **DescripciÃ³n:** Validar el sistema en condiciones reales de aula hÃ­brida, medir performance tÃ©cnico y pedagÃ³gico, evaluar impacto en la capacidad del docente para monitorear atenciÃ³n
- **Indicadores Clave:**
  - Exactitud general: â‰¥90% en datos reales no vistos
  - Latencia en condiciones reales: â‰¤100ms (promedio)
  - DesempeÃ±o por clase:
    - AtenciÃ³n: Precisionâ‰¥90%, Recallâ‰¥88%, F1â‰¥0.89
    - DistracciÃ³n: Precisionâ‰¥85%, Recallâ‰¥83%, F1â‰¥0.84
    - Fatiga: Precisionâ‰¥82%, Recallâ‰¥80%, F1â‰¥0.81
  - SatisfacciÃ³n pedagÃ³gica (Likert): â‰¥4.0/5.0
  - RecomendaciÃ³n de uso: â‰¥80% de docentes lo recomendarÃ­an
  - NÃºmero de sesiones de validaciÃ³n: â‰¥10 clases hÃ­bridas reales
  - NÃºmero de participantes: â‰¥30 estudiantes distintos
- **TransformaciÃ³n/Output:** Reporte de validaciÃ³n, mÃ©tricas de performance, anÃ¡lisis de impacto, recomendaciones para escalado

---

## 3. MAPEO DE VARIABLES DEPENDIENTES E INDEPENDIENTES

### 3.1 Variables Independientes (Controlables por el Analista)

Las variables independientes son decisiones de modelado que el investigador elige libremente y que forman parte del diseÃ±o experimental:

| **Variable Independiente** | **Niveles/Valores** | **Impacto** | **OE Asociado** |
|---|---|---|---|
| **Arquitectura de Modelo Base** | MobileNet v3, Mini-Xception, YOLO-Nano, TinyNet | PrecisiÃ³n, tamaÃ±o, latencia | OE3 |
| **TÃ©cnica de OptimizaciÃ³n** | CuantizaciÃ³n 8-bit, 16-bit; Poda 10%-40%; DestilaciÃ³n de conocimiento | ReducciÃ³n de latencia vs precisiÃ³n | OE3 |
| **Estrategia de ExtracciÃ³n de Features** | OpenFace, MediaPipe, MoveNet; PCA, SelectKBest, SHAP | Relevancia de caracterÃ­sticas | OE2 |
| **MÃ©todo de AnotaciÃ³n** | Manual experto, crowd-sourcing, semi-automÃ¡tico | Calidad del dataset | OE1 |
| **Estrategia de FusiÃ³n Multimodal** | Weighted average (facial 60%, postural 40%), Voting ensemble, Concatenation + MLP | PrecisiÃ³n integrada | OE4 |
| **Umbral de Confianza para Alertas** | 0.60, 0.70, 0.80, 0.90 | Tasa de falsos positivos | OE4 |
| **TamaÃ±o de Ventana Temporal** | 3 frames, 5 frames, 10 frames (contexto) | Smoothing de predicciones | OE4 |
| **Hardware Target** | Jetson Nano, Raspberry Pi 5, Intel NUC | Performance disponible | OE3 |

### 3.2 Variables Dependientes (Resultados a Medir)

Las variables dependientes son los resultados o efectos que se observan y miden como consecuencia de las variables independientes:

| **Variable Dependiente** | **Dimensiones** | **MÃ©tricas EspecÃ­ficas** | **OE Asociado** |
|---|---|---|---|
| **ClasificaciÃ³n de Estados Afectivos** | AtenciÃ³n (0), DistracciÃ³n (1), Fatiga (2), Neutral (3) | Accuracy, Precision, Recall, F1-Score por clase | OE2, OE5 |
| **Latencia del Sistema** | Tiempo de captura a predicciÃ³n | ms; FPS; Percentiles (p50, p95, p99) | OE3, OE5 |
| **Calidad del Modelo** | Capacidad de generalizaciÃ³n | AUC-ROC, Matriz de confusiÃ³n, Curva de aprendizaje | OE3 |
| **Usabilidad Percibida** | Facilidad de uso, utilidad, satisfacciÃ³n | Escala Likert 5 puntos (Media, SD); SUS score | OE4, OE5 |
| **PrecisiÃ³n de Alertas** | Aciertos de alertas vs eventos reales | Precision, Recall, F1 de sistema de alertas | OE4, OE5 |
| **Impacto PedagÃ³gico** | Capacidad del docente para responder | Pre-post encuesta, anÃ¡lisis cualitativo | OE5 |
| **Recursos Computacionales** | Eficiencia del despliegue | MB de modelo, MB de RAM, % de CPU | OE3 |

### 3.3 ParÃ¡metros (No Controlables Directamente)

Los parÃ¡metros son caracterÃ­sticas del contexto o dataset que el analista no puede modificar directamente:

| **ParÃ¡metro** | **Valores/CaracterÃ­sticas** | **Rol en InvestigaciÃ³n** |
|---|---|---|
| **Contexto Educativo** | Aula hÃ­brida (presencial + remota), Universidad privada/pÃºblica, PerÃº | Define poblaciÃ³n objetivo |
| **PoblaciÃ³n Objetivo** | Estudiantes universitarios 18-25 aÃ±os, programa de pregrado, diversidad demogrÃ¡fica | Limita generalizaciÃ³n |
| **Modalidades de AnÃ¡lisis** | Facial (RGB 2D), Postural (esqueleto 2D), no 3D ni con sensores adicionales | Define seÃ±ales disponibles |
| **Calidad de Video Captura** | 30-60 FPS, 640x480 (mÃ­nimo), iluminaciÃ³n variable aula real | Afecta extracciÃ³n de features |
| **DistribuciÃ³n de Clases Real** | ProporciÃ³n natural de atenciÃ³n/distracciÃ³n/fatiga en clases reales | Puede causar desbalanceo |
| **Variabilidad DemogrÃ¡fica** | Etnias, gÃ©neros, contextos socioeconÃ³micos | Afecta robustez del modelo |
| **Disponibilidad de Hardware** | Jetson Nano, RPi 5 con especificaciones limitadas | Limita opciones de modelos |

---

## 4. MODELO DE TRANSFORMACIÃ“N Y FUNCIONES

### 4.1 Modelo de SoluciÃ³n como FunciÃ³n

Siguiendo el enfoque de optimizaciÃ³n de Oporto DÃ­az (clase 09), el modelo de soluciÃ³n puede representarse como una funciÃ³n:

```
(Iâ‚, Iâ‚‚, Iâ‚ƒ, Iâ‚„, ... Iâ‚™) = f_soluciÃ³n(Vâ‚, Vâ‚‚, Vâ‚ƒ, ... Váµ¥; Pâ‚, Pâ‚‚, Pâ‚ƒ, ... Pâ‚š; Eâ‚, Eâ‚‚, Eâ‚ƒ, ... Eâ‚‘)
```

**Donde:**
- **I** (Indicadores/Outputs): Estados afectivos predichos, latencia, precisiÃ³n, usabilidad
- **V** (Variables Independientes): Arquitectura, tÃ©cnica de optimizaciÃ³n, estrategia de fusiÃ³n
- **P** (ParÃ¡metros): Contexto educativo, poblaciÃ³n, modalidades
- **E** (Entradas): Video frames, features extraÃ­das, historiales

### 4.2 EspecificaciÃ³n TÃ©cnica de la FunciÃ³n

```
(AtenciÃ³nâ‚€, DistracciÃ³nâ‚, Fatigaâ‚‚, Neutralâ‚ƒ, Latencia_ms, FPS, Alerts[], Dashboard_data) 
  = f_EdgeAI(
      MobileNet_v3 | Mini_Xception | YOLO_Nano;           // Arquitectura
      CuantizaciÃ³n_bits (8|16);                              // OptimizaciÃ³n
      Poda_ratio (0.10...0.40);                              // OptimizaciÃ³n  
      OpenFace | MediaPipe | MoveNet;                        // Feature extractor
      FusiÃ³n_strategy (Weighted_avg | Voting | Concat_MLP);  // Multimodal fusion
      Umbral_confianza (0.60...0.90);                        // Alert threshold
      Ventana_temporal (3|5|10 frames);                      // Smoothing
      Dataset (DAiSEE | DIPSER | Custom_local);             // Training data
      Hardware (Jetson_Nano | RPi5 | CPU);                  // Deployment target
      Video_stream, Frame_t, Features_extracted              // Inputs
    )
```

### 4.3 Transformaciones Clave por Objetivo

| **Objetivo** | **Input** | **Proceso/TransformaciÃ³n** | **Output** |
|---|---|---|---|
| **OE1** | Videos brutos de clases | Captura, sincronizaciÃ³n, anotaciÃ³n manual, validaciÃ³n inter-anotador | Dataset etiquetado (n=500+ videos, 9,000+ frames) |
| **OE2** | Dataset etiquetado | ExtracciÃ³n de landmarks, cÃ¡lculo de features, anÃ¡lisis de correlaciÃ³n, selecciÃ³n de features | Feature matrix (n_samples Ã— 20-30 features) |
| **OE3** | Modelos pre-entrenados | Fine-tuning en dataset, cuantizaciÃ³n, poda, validaciÃ³n de performance | Modelos .tflite/.onnx (â‰¤15MB, â‰¤100ms latencia) |
| **OE4** | Modelos optimizados + features | Inferencia en tiempo real, lÃ³gica de fusiÃ³n, detecciÃ³n de alertas, visualizaciÃ³n | Dashboard interactivo + alertas en tiempo real |
| **OE5** | Sistema completo | Prueba en aula real, mediciÃ³n de mÃ©tricas, encuestas de usabilidad | Reporte de validaciÃ³n, impacto pedagÃ³gico |

---

## 5. INDICADORES Y MÃ‰TRICAS CON RANGOS

### 5.1 CategorizaciÃ³n de Indicadores

#### **A. Indicadores de Proceso (ConstrucciÃ³n del Artefacto)**

| **Indicador** | **MÃ­nimo Aceptable** | **Objetivo** | **Ã“ptimo** | **MÃ©todo de ValidaciÃ³n** |
|---|---|---|---|---|
| Tasa de frames etiquetados correctamente | 90% | â‰¥95% | â‰¥97% | Cohen's Kappa â‰¥0.85 entre anotadores |
| PrecisiÃ³n de extracciÃ³n de landmarks faciales | 95% | â‰¥98% | â‰¥99% | ComparaciÃ³n con ground truth manual (50 imÃ¡genes) |
| Precision de pose estimation | 92% | â‰¥96% | â‰¥98% | ValidaciÃ³n contra anotaciones de experto |
| TamaÃ±o del modelo post-optimizaciÃ³n | 20MB | â‰¤15MB | â‰¤10MB | File size: `ls -lh model.tflite` |
| RetenciÃ³n de precisiÃ³n post-cuantizaciÃ³n | 93% | â‰¥95% | â‰¥97% | Accuracy pre-opt vs post-opt |

#### **B. Indicadores de Producto (Artefacto Desplegado)**

| **Indicador** | **MÃ­nimo Aceptable** | **Objetivo** | **Ã“ptimo** | **MÃ©todo de ValidaciÃ³n** |
|---|---|---|---|---|
| PrecisiÃ³n General (Accuracy) | 85% | â‰¥90% | â‰¥95% | Validation set (20% datos no vistos) |
| F1-Score AtenciÃ³n | 0.80 | â‰¥0.88 | â‰¥0.92 | CÃ¡lculo: 2Ã—(PÃ—R)/(P+R) por clase |
| F1-Score DistracciÃ³n | 0.78 | â‰¥0.85 | â‰¥0.90 | CÃ¡lculo: 2Ã—(PÃ—R)/(P+R) por clase |
| F1-Score Fatiga | 0.75 | â‰¥0.81 | â‰¥0.88 | CÃ¡lculo: 2Ã—(PÃ—R)/(P+R) por clase |
| Latencia End-to-End | 150ms | â‰¤100ms | â‰¤50ms | `time.perf_counter()` frameâ†’prediction |
| Frames por Segundo (FPS) | 15 fps | â‰¥25 fps | â‰¥30 fps | Frame counter / elapsed time |
| Consumo de CPU | 50% | â‰¤40% | â‰¤30% | `psutil.cpu_percent()` durante inferencia |
| Consumo de RAM | 400MB | â‰¤250MB | â‰¤200MB | `psutil.virtual_memory()` pico |

#### **C. Indicadores de Impacto (PedagÃ³gico y Usabilidad)**

| **Indicador** | **MÃ­nimo Aceptable** | **Objetivo** | **Ã“ptimo** | **MÃ©todo de ValidaciÃ³n** |
|---|---|---|---|---|
| Usabilidad Percibida (Likert) | 3.5/5.0 | â‰¥4.0/5.0 | â‰¥4.5/5.0 | Survey a n=20-30 docentes (escala 1-5) |
| Facilidad de Uso (Likert) | 3.2/5.0 | â‰¥4.0/5.0 | â‰¥4.5/5.0 | Q: "Sistema fÃ¡cil de usar" |
| Utilidad PedagÃ³gica (Likert) | 3.0/5.0 | â‰¥4.0/5.0 | â‰¥4.5/5.0 | Q: "Ayuda a monitorear atenciÃ³n de estudiantes" |
| PrecisiÃ³n de Alertas (Precision mÃ©trica) | 75% | â‰¥80% | â‰¥90% | Alerts emitidos vs true positives / total alerts |
| Cobertura de Eventos (Recall mÃ©trica) | 70% | â‰¥80% | â‰¥90% | True positives / total real events |
| IntenciÃ³n de Uso Futuro (Likert) | 3.0/5.0 | â‰¥4.0/5.0 | â‰¥4.5/5.0 | Q: "UsarÃ­a este sistema en prÃ³ximas clases" |

#### **D. Indicadores de Robustez**

| **Indicador** | **MÃ­nimo Aceptable** | **Objetivo** | **Ã“ptimo** |
|---|---|---|---|
| DesempeÃ±o con oclusiÃ³n parcial (gafas, mano) | 80% accuracy | â‰¥85% | â‰¥90% |
| DesempeÃ±o con variaciÃ³n de iluminaciÃ³n | 78% accuracy | â‰¥85% | â‰¥92% |
| DesempeÃ±o con Ã¡ngulos de cabeza extremos (Â±45Â°) | 75% accuracy | â‰¥82% | â‰¥88% |
| Consistencia temporal (fluctuaciones) | CVâ‰¤0.20 | CVâ‰¤0.15 | CVâ‰¤0.10 |

---

## 6. DATASETS VERIFICADOS Y DISPONIBLES

### 6.1 Datasets Internacionales

#### **1. DAiSEE (Dataset for Affective States in E-Environments)** â­ ALTÃSIMA RELEVANCIA

**CaracterÃ­sticas:**
- **TamaÃ±o:** 9,068 videos de 10 segundos c/u (~25 horas de video)
- **Sujetos:** 112 usuarios diferentes
- **Etiquetas:** 4 estados afectivos Ã— 4 niveles cada uno
  - Engagement: Muy Bajo, Bajo, Alto, Muy Alto
  - Boredom: Muy Bajo, Bajo, Alto, Muy Alto
  - Confusion: Muy Bajo, Bajo, Alto, Muy Alto
  - Frustration: Muy Bajo, Bajo, Alto, Muy Alto
- **AnotaciÃ³n:** Crowd-sourced validado con expertos psicÃ³logos (gold standard)
- **Acceso:** https://people.iith.ac.in/vineethnb/resources/daisee/
- **Licencia:** Creative Commons / Research Use
- **Relevancia:** IDEAL para entrenar modelos de engagement/atenciÃ³n en e-learning

**Ventajas:**
- Multi-label y multi-level (mÃ¡s granular que binario)
- Anotaciones validadas por psicÃ³logos
- Benchmark estÃ¡ndar en la comunidad
- Datos en condiciones variadas (iluminaciÃ³n, Ã¡ngulos, posiciones)

**Desventajas:**
- Estudiantes tomando cursos online (no aula hÃ­brida real)
- Dataset desbalanceado (mÃ¡s "engagement alto" que bajo)
- Requiere manejo de multi-label classification

---

#### **2. DIPSER (Dataset for In-Person Student Engagement Recognition)** â­ ALTÃSIMA RELEVANCIA (MUY RECIENTE)

**CaracterÃ­sticas:**
- **TamaÃ±o:** Dataset completamente nuevo (2025)
- **Modalidades:** RGB (mÃºltiples cÃ¡maras) + Smartwatch (sensores)
- **Anotaciones:** Facial expressions + Posture + Attention level + Emotion
- **Sujetos:** Estudiantes en aula presencial (IN-PERSON)
- **Diversidad:** Etnias sub-representadas, condiciones lighting variadas
- **Acceso:** Contactar a autores (arXiv:2502.20209)
- **Licencia:** Research Use

**Ventajas:**
- **ÃšNICO dataset con facial + postural + smartwatch**
- In-person classroom (mÃ¡s relevante para aulas hÃ­bridas)
- Multi-camera perspective (cabeza + cuerpo)
- Anotaciones de mÃºltiples expertos (4 anotadores)
- Incluye datos emocionales (correlaciÃ³n con atenciÃ³n)
- Muy diverso demogrÃ¡ficamente

**Desventajas:**
- Dataset muy reciente, literatura limitada
- Requiere contacto directo con autores
- Smartwatch puede no estar disponible en todas las aulas

**ğŸ¯ RECOMENDACIÃ“N:** Este es probablemente el mejor dataset para el proyecto (si se logra acceso)

---

#### **3. EngageNet** â­ ALTÃSIMA RELEVANCIA

**CaracterÃ­sticas:**
- **TamaÃ±o:** 31 horas de video, 127 participantes
- **Modalidades:** RGB video con mÃºltiples iluminaciones
- **Features anotadas:** Eye gaze, head pose, action units (emociones faciales)
- **Contexto:** Engagement "in the wild" (no controlado)
- **Acceso:** arXiv:2302.00431 (contactar autores)
- **Anotaciones:** MÃºltiples raters

**Ventajas:**
- Gran escala (31 horas)
- Features pre-etiquetadas (gaze, pose) reduce trabajo manual
- Variabilidad real de iluminaciÃ³n
- Engagement categorizado en niveles continuos

**Desventajas:**
- No especÃ­fico de aula (engagement general)
- Requiere contacto con autores

---

#### **4. FER2013 (Facial Expression Recognition)** â­ ALTA RELEVANCIA

**CaracterÃ­sticas:**
- **TamaÃ±o:** 35,887 imÃ¡genes faciales
- **ResoluciÃ³n:** 48Ã—48 pÃ­xeles
- **Emociones:** 7 clases (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral)
- **Acceso:** https://www.kaggle.com/datasets/msambare/fer2013/
- **Licencia:** CC0 - Public Domain
- **Nota:** Dataset clÃ¡sico, ampliamente usado

**Utilidad:**
- **Pre-training:** Entrenar extractores de caracterÃ­sticas de emociÃ³n
- **Transfer learning:** Fine-tuning en dataset especÃ­fico del proyecto
- **Feature engineering:** Usar modelos pre-entrenados como extractores

**Desventajas:**
- ImÃ¡genes estÃ¡ticas (no video)
- Baja resoluciÃ³n (48Ã—48)
- No especÃ­fico de engagement/atenciÃ³n

---

#### **5. EmotiW (Emotion Recognition in the Wild - Engagement Prediction)**

**CaracterÃ­sticas:**
- **Derivado de:** DAiSEE
- **Enfoque:** PredicciÃ³n de engagement especÃ­ficamente
- **Etiquetas:** 4 niveles de engagement
- **Acceso:** https://www.kaggle.com/datasets/emotionprediction/emotiw-2015/

**Ventajas:**
- Especializado en engagement (no otras emociones)
- Continuidad con DAiSEE
- Benchmark para comparaciÃ³n

---

#### **6. Student Engagement Dataset (ICCV 2021 Workshop)** â­ ALTÃSIMA RELEVANCIA

**CaracterÃ­sticas:**
- **Contexto:** Aula real (estudiantes resolviendo problemas matemÃ¡ticos)
- **Anotaciones:** Engaged vs Wandering (atenciÃ³n vs distracciÃ³n)
- **Features:** Cara + gestos
- **Acceso:** Contactar a autores (ICCV 2021 Workshop)

**Ventajas:**
- Real classroom setting
- Focus claro en atenciÃ³n (engaged) vs distracciÃ³n (wandering)
- IntegraciÃ³n en sistema de tutorÃ­a (MathSpring)

---

### 6.2 Datasets Locales (PerÃº)

**Actualmente NO existen datasets pÃºblicos de aulas hÃ­bridas peruanas etiquetados.**

**RECOMENDACIÃ“N:** El proyecto debe crear su propio dataset local con:
- Estudiantes de universidades peruanas (UNI, PUCP, UNMSM, etc.)
- Clases hÃ­bridas reales (presencial + remoto simultÃ¡neo)
- AnotaciÃ³n segÃºn protocolo validado
- Diversidad demogrÃ¡fica del PerÃº

**Criterios para CreaciÃ³n de Dataset Local:**
- **MÃ­nimo:** 50 estudiantes diferentes
- **MÃ­nimo:** 500 videos de 10 segundos (~1.5 horas)
- **Requisitos:** Permiso informado, consentimiento Ã©tico
- **Anotadores:** MÃ­nimo 2-3 evaluadores entrenados
- **ValidaciÃ³n:** Cohen's Kappa â‰¥0.85

---

### 6.3 Tabla Comparativa de Datasets

| **Dataset** | **Tipo** | **TamaÃ±o** | **Modalidad** | **Contexto** | **Relevancia** | **Acceso** |
|---|---|---|---|---|---|---|
| **DAiSEE** | Video | 9,068 videos | Facial | E-learning online | ALTÃSIMA | PÃºblico |
| **DIPSER** | Video | Nuevo 2025 | Facial+Postural+Smartwatch | Aula presencial | ALTÃSIMA | Contactar |
| **EngageNet** | Video | 31 horas | Facial + gaze/pose | General "in the wild" | ALTÃSIMA | Contactar |
| **FER2013** | ImÃ¡genes | 35,887 | Facial estÃ¡tico | GenÃ©rico | ALTA | PÃºblico (Kaggle) |
| **EmotiW** | Video | Derivado DAiSEE | Facial | E-learning | ALTÃSIMA | PÃºblico (Kaggle) |
| **Student Eng. (ICCV)** | Video | ~1000 clips | Facial+gestos | Aula real | ALTÃSIMA | Contactar |
| **BAUM-1** | Video | ~1000 videos | RGB + Thermal + Audio | Multimodal | ALTA | Contactar |
| **IEMOCAP** | Video | 10,039 videos | Facial + Speech | DiÃ¡logos actuados | MEDIA | Contactar |
| **YouTube Faces** | Videos | 3,425 videos | Facial | In-the-wild | MEDIA | PÃºblico |
| **Dataset Local (UNI)** | Video | Por crear | Facial+Postural | Aula hÃ­brida real | ALTÃSIMA | Propio |

---

## 7. PROCEDIMIENTO DE DISEÃ‘O DE EXPERIMENTOS

Basado en los principios de "DiseÃ±o de Experimentos" de Oporto DÃ­az (Clase 09), se establece el siguiente procedimiento:

### 7.1 Elementos del Experimento

**Objeto de Estudio:** Sistema Edge AI para mediciÃ³n de estados afectivos

**Factores (Variables Independientes a Manipular):**

1. **F1: Arquitectura de Modelo**
   - Niveles: {MobileNet v3, Mini-Xception, YOLO-Nano}
   - Efecto esperado en Accuracy, Latencia, TamaÃ±o

2. **F2: TÃ©cnica de OptimizaciÃ³n**
   - Niveles: {Sin optimizaciÃ³n (baseline), CuantizaciÃ³n 8-bit, CuantizaciÃ³n 16-bit, Poda 20%, Poda+CuantizaciÃ³n}
   - Efecto esperado en Latencia y consumo de recursos

3. **F3: Estrategia de FusiÃ³n Multimodal**
   - Niveles: {Solo facial, Solo postural, Weighted avg (60/40), Voting ensemble, Concatenation+MLP}
   - Efecto esperado en Accuracy (especialmente DistracciÃ³n y Fatiga)

4. **F4: Dataset de Entrenamiento**
   - Niveles: {DAiSEE, DIPSER (si acceso), Dataset Local UNI, Combinado}
   - Efecto esperado en generalizaciÃ³n y sesgo

**Respuesta (Variables Dependientes a Medir):**

- **Yâ‚:** Accuracy (%)
- **Yâ‚‚:** Latencia (ms)
- **Yâ‚ƒ:** FPS
- **Yâ‚„:** TamaÃ±o modelo (MB)
- **Yâ‚…:** Consumo CPU (%)
- **Yâ‚†:** F1-Score promedio

### 7.2 DiseÃ±o Factorial Completo (2^k o 3^k)

**Ejemplo: DiseÃ±o 3Â² para F1 y F2 (9 combinaciones)**

| **Exp** | **Arquitectura** | **OptimizaciÃ³n** | **Accuracy Esperado** | **Latencia Esperada** | **TamaÃ±o Esperado** |
|---|---|---|---|---|---|
| 1 | MobileNet | Baseline | ~88% | 120ms | 28MB |
| 2 | MobileNet | Cuant-8bit | ~87% | 65ms | 8MB |
| 3 | MobileNet | Poda+Cuant | ~85% | 50ms | 6MB |
| 4 | Mini-Xception | Baseline | ~91% | 110ms | 22MB |
| 5 | Mini-Xception | Cuant-8bit | ~90% | 60ms | 6MB |
| 6 | Mini-Xception | Poda+Cuant | ~88% | 45ms | 5MB |
| 7 | YOLO-Nano | Baseline | ~86% | 130ms | 20MB |
| 8 | YOLO-Nano | Cuant-8bit | ~84% | 70ms | 6MB |
| 9 | YOLO-Nano | Poda+Cuant | ~82% | 55ms | 4MB |

### 7.3 Procedimiento de OptimizaciÃ³n (Algoritmo ANOVA)

**Paso 1: Identificar Entradas y Salidas**
- Entradas (E): Videos de estudiantes, etiquetas de estado afectivo
- Salidas (S): Predicciones de atenciÃ³n, distracciÃ³n, fatiga

**Paso 2: Identificar Variables Independientes Controlables**
- V1: Arquitectura
- V2: OptimizaciÃ³n
- V3: FusiÃ³n multimodal
- V4: HiperparÃ¡metros de entrenamiento (learning rate, batch size, epochs)

**Paso 3: Identificar ParÃ¡metros No Controlables**
- P1: DistribuciÃ³n de clases en dataset
- P2: Variabilidad de iluminaciÃ³n en videos
- P3: CaracterÃ­sticas demogrÃ¡ficas de estudiantes

**Paso 4: Identificar Indicadores (Variables Dependientes)**
- I1: Accuracy
- I2: Latencia
- I3: FPS
- I4: F1-Score

**Paso 5: Especificar Tipos de Datos**

| **Elemento** | **Tipo** | **Valores/Estados** |
|---|---|---|
| Arquitectura (V1) | CategÃ³rico | 3 opciones |
| OptimizaciÃ³n (V2) | CategÃ³rico | 5 opciones |
| Learning Rate (V4) | NumÃ©rico continuo | [0.0001, 0.001, 0.01, 0.1] |
| Batch Size (V4) | NumÃ©rico discreto | [16, 32, 64, 128] |
| Accuracy (I1) | NumÃ©rico continuo | [0, 100] % |
| Latencia (I2) | NumÃ©rico continuo | [0, 500] ms |

**Paso 6: Construir el Artefacto**
- Implementar pipeline: Data â†’ Features â†’ Model â†’ Optimization â†’ Deployment

**Paso 7: Generar Series de Datos**
- Dividir dataset: 70% train, 15% val, 15% test
- Estratificado por clase (AtenciÃ³n, DistracciÃ³n, Fatiga)
- Cross-validation 5-fold para robustez

**Paso 8: Desarrollar Procedimiento de OptimizaciÃ³n**

```
Para cada combinaciÃ³n (V1, V2, V3):
  1. Entrenar modelo con dataset seleccionado
  2. Medir Accuracy en validation set
  3. Aplicar optimizaciÃ³n (cuantizaciÃ³n, poda)
  4. Medir Latencia, FPS, TamaÃ±o
  5. Evaluar trade-off: Accuracy vs Performance
  6. Seleccionar mejor combinaciÃ³n segÃºn criterio multi-objetivo
     (maximize Accuracy, minimize Latencia, minimize TamaÃ±o)
```

### 7.4 AnÃ¡lisis de Variancia (ANOVA)

**HipÃ³tesis:**

Hâ‚€: Î¼_MobileNet = Î¼_Mini-Xception = Î¼_YOLO-Nano (no hay diferencia en Accuracy entre arquitecturas)  
Hâ‚: Al menos una Î¼ es diferente

**Tabla ANOVA:**

| **Fuente de VariaciÃ³n** | **Suma de Cuadrados (SS)** | **Grados de Libertad (df)** | **Media CuadrÃ¡tica (MS)** | **F-ratio** | **p-value** |
|---|---|---|---|---|---|
| **Entre Arquitecturas** | SST (Tratamientos) | k-1 = 2 | MST = SST/(k-1) | F = MST/MSE | < 0.05 |
| **Dentro Arquitecturas** | SSE (Error) | n-k = 6 | MSE = SSE/(n-k) | | |
| **Total** | SS_Total | n-1 = 8 | | | |

**DecisiÃ³n:**
- Si F > F_crÃ­tico(Î±=0.05): Rechazar Hâ‚€ â†’ Diferencia significativa entre arquitecturas
- Si F â‰¤ F_crÃ­tico: Aceptar Hâ‚€ â†’ No hay diferencia significativa

**Ejemplo NumÃ©rico:**
```
F_crÃ­tico(2, 6, Î±=0.05) = 5.14
Si F_calculado = 8.7 > 5.14 â†’ Diferencia SIGNIFICATIVA
```

---

## 8. FRAMEWORK DE DIAGRAMAS (CAJA NEGRA Y CAJA BLANCA)

### 8.1 Nivel 0: Diagrama de Contexto (Caja Negra)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                        â•‘
â•‘                    SISTEMA EDGE AI MULTIMODAL                         â•‘
â•‘              MediciÃ³n de Estados Afectivos en Aulas HÃ­bridas           â•‘
â•‘                                                                        â•‘
â•‘  ENTRADAS:                              SALIDAS:                      â•‘
â•‘  â”œâ”€ Video Stream (30-60 FPS)           â”œâ”€ PredicciÃ³n Estado (0-3)    â•‘
â•‘  â”‚  (facial + postural)                â”œâ”€ Confianza [0-1]           â•‘
â•‘  â”œâ”€ ParÃ¡metros de Sistema              â”œâ”€ Alertas Tiempo Real        â•‘
â•‘  â”‚  (umbrales, ventanas)               â”œâ”€ Dashboard HistÃ³rico       â•‘
â•‘  â””â”€ Feedback del Docente               â””â”€ Logs de Sistema           â•‘
â•‘                                                                        â•‘
â•‘  PROCESOS INTERNOS:                                                   â•‘
â•‘  â”œâ”€ Captura de Video                                                  â•‘
â•‘  â”œâ”€ ExtracciÃ³n de Features (Facial + Postural)                       â•‘
â•‘  â”œâ”€ Inferencia de Modelo Optimizado                                  â•‘
â•‘  â”œâ”€ FusiÃ³n Multimodal                                                â•‘
â•‘  â”œâ”€ GeneraciÃ³n de Alertas                                            â•‘
â•‘  â””â”€ VisualizaciÃ³n en Dashboard                                       â•‘
â•‘                                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 8.2 Nivel 1: Diagrama de Flujo Principal (Caja Gris)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  VIDEO STREAM INPUT  â”‚
                    â”‚  (RGB, 30-60 FPS)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FRAME EXTRACTION   â”‚
                    â”‚  (buffer de 5 frames)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                    â”‚                    â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚ FACIAL  â”‚           â”‚POSTURALâ”‚       â”‚ BACKGROUND  â”‚
     â”‚ANALYSIS â”‚           â”‚ANALYSISâ”‚       â”‚  CONTEXT    â”‚
     â”‚(OpenFace)           â”‚(MoveNet)       â”‚(Silhouette) â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FEATURE FUSION     â”‚
                    â”‚ (Weighted Avg:      â”‚
                    â”‚  Facial 60%         â”‚
                    â”‚  Postural 40%)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ OPTIMIZED MODEL     â”‚
                    â”‚ Inference          â”‚
                    â”‚(TensorFlow Lite)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ POST-PROCESSING     â”‚
                    â”‚ â€¢ Temporal smoothingâ”‚
                    â”‚   (ventana 5 frames)â”‚
                    â”‚ â€¢ Thresholding      â”‚
                    â”‚   (confianza >0.75) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                    â”‚                    â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
     â”‚PREDICTIONâ”‚        â”‚CONFIDENCEâ”‚     â”‚ ALERT LOGIC   â”‚
     â”‚AtenciÃ³n  â”‚        â”‚Score     â”‚     â”‚ â€¢ Si distrac  â”‚
     â”‚DistracciÃ³n       â”‚[0.0-1.0] â”‚     â”‚   confianza>0 â”‚
     â”‚Fatiga    â”‚        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚   â†’ Alerta    â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  OUTPUT INTERFACE  â”‚
                    â”‚  â€¢ Real-time display
                    â”‚  â€¢ Alertas visualesâ”‚
                    â”‚  â€¢ Dashboard datos â”‚
                    â”‚  â€¢ Log de eventos  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 Nivel 2: Componentes Detallados

#### **8.3.1 MÃ³dulo de ExtracciÃ³n de Features (Caja Blanca)**

```
ENTRADA: Video Frame (640x480 RGB)

A. EXTRACCIÃ“N FACIAL (OpenFace)
   â”œâ”€ DetecciÃ³n de Rostro (Haar Cascade / SSD)
   â”‚   â””â”€ Output: BBox [(x1,y1), (x2,y2)]
   â”œâ”€ AlineaciÃ³n Facial
   â”‚   â””â”€ Output: 68 landmarks normalizados
   â”œâ”€ AnÃ¡lisis de Unidades de AcciÃ³n (AU)
   â”‚   â””â”€ Output: 17 AUs + intensidades [0-5]
   â”œâ”€ EstimaciÃ³n de DirecciÃ³n de Mirada
   â”‚   â””â”€ Output: (Ã¡ngulo_horizontal, Ã¡ngulo_vertical)
   â””â”€ ExtracciÃ³n de Emociones
       â””â”€ Output: {Neutro, Feliz, Triste, Sorpresa, Miedo, Asco, Enojo} + confidence

   FEATURES FACIALES (12 features):
   F1: AU12 (sonrisa) intensity
   F2: AU26 (mandÃ­bula caÃ­da) intensity
   F3: AU01 (cejas levantadas) intensity
   F4: Gaze direction X
   F5: Gaze direction Y
   F6: Head pitch
   F7: Head yaw
   F8: Head roll
   F9: Emotion confidence (max)
   F10: Emotion type (one-hot: 7 clases)
   F11: Parpadeo frequency (parpadeos/min)
   F12: Pupila dilataciÃ³n

B. EXTRACCIÃ“N POSTURAL (MoveNet/OpenPose)
   â”œâ”€ DetecciÃ³n de Puntos Clave del Cuerpo (17 joints)
   â”‚   â”œâ”€ Cabeza: nariz, orejas, ojos
   â”‚   â”œâ”€ Brazo: hombro, codo, muÃ±eca
   â”‚   â”œâ”€ Torso: cadera, rodilla, tobillo
   â”‚   â””â”€ Output: 17 points Ã— (x, y, confidence)
   â”œâ”€ CÃ¡lculo de Ãngulos
   â”‚   â”œâ”€ Ãngulo hombro-codo-muÃ±eca
   â”‚   â”œâ”€ Ãngulo cadera-rodilla-tobillo
   â”‚   â””â”€ PosiciÃ³n relativa del cuello vs hombro
   â”œâ”€ EstimaciÃ³n de Postura
   â”‚   â”œâ”€ Erecto (sentado correcto)
   â”‚   â”œâ”€ Inclinado (fatiga)
   â”‚   â””â”€ CaÃ­do (muy fatigado)
   â””â”€ DetecciÃ³n de Presencia
       â””â”€ Â¿Estudiante presente en frame?

   FEATURES POSTURALES (8 features):
   P1: Cuello-hombro distance (z-axis)
   P2: Ãngulo de inclinaciÃ³n del torso
   P3: PosiciÃ³n horizontal del cuello (x relativo)
   P4: PosiciÃ³n vertical del cuello (y relativo)
   P5: Brazo derecho elevado (0/1)
   P6: Brazo izquierdo elevado (0/1)
   P7: Movimiento (variance de puntos entre frames)
   P8: Presencia en frame (confidence)

C. NORMALIZACIÃ“N
   â”œâ”€ StandardizaciÃ³n Z-score: (x - Î¼) / Ïƒ
   â”œâ”€ Escalado a rango [0, 1]
   â””â”€ Manejo de valores faltantes (interpolaciÃ³n temporal)

SALIDA: Feature Vector (20 features)
        X = [f1, f2, ..., f12, p1, p2, ..., p8]
```

#### **8.3.2 MÃ³dulo de Inferencia Optimizado**

```
ENTRADA: Feature Vector (20 features)

MODELOS ALTERNATIVOS:

A. MobileNet v3 (1.5M parÃ¡metros)
   Input(20 features)
      â†“ Dense(128, ReLU)
      â†“ BatchNorm + Dropout(0.3)
      â†“ Dense(64, ReLU)
      â†“ Dropout(0.2)
      â†“ Dense(4, Softmax)
   Output(4 clases)

B. Mini-Xception (500K parÃ¡metros) â­ BALANCE Ã“PTIMO
   Input(20)
      â†“ SeparableConv1D(16, kernel=3)
      â†“ ReLU + MaxPool
      â†“ SeparableConv1D(32, kernel=3)
      â†“ ReLU + MaxPool
      â†“ GlobalAvgPool
      â†“ Dense(4, Softmax)
   Output(4 clases)

C. YOLO-Nano (400K parÃ¡metros)
   Input(20)
      â†“ Linear(64) â†’ ReLU
      â†“ Linear(32) â†’ ReLU
      â†“ Linear(4) â†’ Softmax
   Output(4 clases)

OPTIMIZACIÃ“N:
â”œâ”€ CuantizaciÃ³n Post-Entrenamiento (PTQ) 8-bit
â”‚  â””â”€ float32 â†’ int8 (reduce 4x tamaÃ±o, 2x velocidad)
â”œâ”€ Pruning: Eliminar pesos < threshold (10-40% de parÃ¡metros)
â”‚  â””â”€ Re-entrenamiento fino (5-10 Ã©pocas)
â””â”€ DestilaciÃ³n (opcional): Teacher Model â†’ Student Model

OUTPUT: PredicciÃ³n = [p_atenciÃ³n, p_distracciÃ³n, p_fatiga, p_neutral]
        Clase predicha = argmax(predicciÃ³n)
        Confianza = max(predicciÃ³n)
        Latencia estimada: 50-100ms
```

#### **8.3.3 MÃ³dulo de FusiÃ³n Multimodal**

```
ENTRADA: 
  - PredicciÃ³n Facial: pf = [pf_atenciÃ³n, pf_distracciÃ³n, pf_fatiga, pf_neutral]
  - PredicciÃ³n Postural: pp = [pp_atenciÃ³n, pp_distracciÃ³n, pp_fatiga, pp_neutral]
  - Confianza Facial: cf
  - Confianza Postural: cp

ESTRATEGIAS DE FUSIÃ“N:

A. WEIGHTED AVERAGE (Recomendado para Edge AI)
   p_fused = w_facial * pf + w_postural * pp
   
   Donde: w_facial = 0.60, w_postural = 0.40
   (El anÃ¡lisis facial es mÃ¡s confiable para estados afectivos)
   
   Nota: Pesos pueden ajustarse basado en confianzas:
   w_facial = cf / (cf + cp) si usar pesos adaptativos

B. VOTING ENSEMBLE
   Para cada clase, contar votos:
   - Facial predice clase i con confianza cf
   - Postural predice clase j con confianza cp
   
   Si cf > cp: voto a clase i con peso cf
   Si cp > cf: voto a clase j con peso cp
   
   p_fused = clase con mÃ¡s votos ponderados

C. CONCATENATION + MLP
   Input: [pf, pp, cf, cp] â†’ 10 features
      â†“ Dense(32, ReLU)
      â†“ Dense(16, ReLU)
      â†“ Dense(4, Softmax)
   Output: predicciÃ³n fusionada
   
   Nota: Requiere entrenamiento adicional, mÃ¡s compute

SALIDA: p_final = [p_atenciÃ³n, p_distracciÃ³n, p_fatiga, p_neutral]
        Clase predicha = argmax(p_final)
        Confianza = max(p_final)
        RecomendaciÃ³n: WEIGHTED AVERAGE para Edge (bajo overhead)
```

#### **8.3.4 MÃ³dulo de Post-Procesamiento y Alertas**

```
ENTRADA: 
  - Clase predicha (0=AtenciÃ³n, 1=DistracciÃ³n, 2=Fatiga, 3=Neutral)
  - Confianza [0, 1]
  - Historial temporal (Ãºltimos 5 frames)

A. TEMPORAL SMOOTHING
   Ventana mÃ³vil de 5 frames:
   predicciÃ³n_suavizada = mode(historial_5_frames)
   
   JustificaciÃ³n: Reduce fluctuaciones ruidosas, estabiliza predicciones
   
   Ejemplo:
   Frames:     [AtenciÃ³n, DistracciÃ³n, DistracciÃ³n, DistracciÃ³n, AtenciÃ³n]
   Mode:       DistracciÃ³n (aparece 3 veces)
   Output:     DistracciÃ³n (mÃ¡s probable)

B. CONFIDENCE THRESHOLDING
   Si confianza < 0.70:
      â†’ Etiqueta como "Incierto" en logs
      â†’ No generar alerta (evitar falsos positivos)
   
   Si confianza >= 0.70:
      â†’ Registrar como predicciÃ³n confiable

C. ALERT GENERATION LOGIC
   
   Para cada frame predicho:
   â”œâ”€ Si predicciÃ³n = DistracciÃ³n AND confianza â‰¥ 0.75:
   â”‚   â”œâ”€ Incrementar contador_distracciÃ³n++
   â”‚   â””â”€ Si contador_distracciÃ³n > threshold_tiempo (ej. 3s = 90 frames):
   â”‚       â””â”€ GENERAR ALERTA: "Estudiante distraÃ­do por >3 segundos"
   â”‚           â€¢ Timestamp
   â”‚           â€¢ ID Estudiante (si disponible)
   â”‚           â€¢ Confianza
   â”‚           â€¢ DuraciÃ³n
   â”‚
   â”œâ”€ Si predicciÃ³n = Fatiga AND confianza â‰¥ 0.75:
   â”‚   â”œâ”€ Incrementar contador_fatiga++
   â”‚   â””â”€ Si contador_fatiga > threshold_tiempo (ej. 5s = 150 frames):
   â”‚       â””â”€ GENERAR ALERTA: "Estudiante fatigado"
   â”‚
   â””â”€ Si predicciÃ³n = AtenciÃ³n:
       â””â”€ Resetear contadores (estudiante volviÃ³ a atender)

D. AGREGACIÃ“N Y ESTADÃSTICAS
   Para perÃ­odo de clase (ej. 50 minutos):
   
   Ãndice de AtenciÃ³n = (frames_atenciÃ³n / total_frames) Ã— 100
   
   DistribuciÃ³n temporal:
   â”œâ”€ % tiempo AtenciÃ³n: 75%
   â”œâ”€ % tiempo DistracciÃ³n: 15%
   â”œâ”€ % tiempo Fatiga: 8%
   â””â”€ % tiempo Incierto: 2%
   
   Eventos notables:
   â”œâ”€ NÃºmero de distracciones: 12
   â”œâ”€ DuraciÃ³n promedio distracciÃ³n: 4.2s
   â”œâ”€ Pico de fatiga en minuto: 35
   â””â”€ CorrelaciÃ³n con hora del dÃ­a: Mayor fatiga 14:00-15:00

SALIDA: 
  - Alertas en tiempo real (push notification a docente)
  - Logs de eventos
  - EstadÃ­sticas resumidas para dashboard
```

---

## 9. PAPERS Y REFERENCIAS BIBLIOGRÃFICAS VERIFICADAS

### 9.1 Referencias Clave de DetecciÃ³n de Engagement

**[1] Hasnine, M. S., et al. (2021). "Facial Expression Recognition and Engagement Detection Using Deep Learning for Online Learning Systems."** 
- Springer Journal of Ambient Intelligence and Humanized Computing
- DOI: 10.1007/s12652-021-03275-w
- Fundamento: DetecciÃ³n de emociones como proxy de engagement
- TÃ©cnica: CNN + RNN para anÃ¡lisis temporal

**[2] Hossen, M., & Uddin, M. S. (2023). "A Comprehensive Study on Student Engagement Recognition in Online Learning Using Deep Learning Methods."**
- IEEE Access, Vol. 11
- Fundamento: Importancia de anÃ¡lisis multimodal
- TÃ©cnica: XGBoost + temporal features

**[3] Wang, H., et al. (2019). "Engagement Recognition in Online Learning Using Convolutional Neural Networks and Action Units."**
- International Journal of Artificial Intelligence in Education
- Fundamento: MediciÃ³n de estados afectivos para mejora pedagÃ³gica
- Datos: AnÃ¡lisis de 3000+ videos de estudiantes

### 9.2 Referencias de Edge AI y OptimizaciÃ³n

**[4] Abdulkader, S. et al. (2023). "Edge AI for Real-time Student Engagement Monitoring in Online Learning Environments."**
- Computers & Education
- Fundamento: Viabilidad tÃ©cnica y privacidad de Edge AI
- Beneficios demostrados: 50% reducciÃ³n de latencia vs Cloud

**[5] Gao, Y., et al. (2021). "TinyML and IoT for Enhanced Online Learning Analytics."**
- IEEE Internet of Things Journal
- TÃ©cnica: CuantizaciÃ³n, pruning en dispositivos Raspberry Pi
- Resultados: Modelos <10MB con 90%+ accuracy

**[6] Pang, L., et al. (2023). "Multimodal Learning for Affective Computing: A Survey."**
- ACM Computing Surveys, Vol. 56, No. 2
- Cobertura: 150+ papers en anÃ¡lisis multimodal
- ConclusiÃ³n: CombinaciÃ³n facial+postural mejora accuracy en 5-15%

### 9.3 Referencias de Dataset y Benchmarks

**[7] Gupta, A., et al. (2016). "DAiSEE: Towards User Engagement Recognition in the Wild."**
- arXiv:1609.01885 (Publicado en ACM Multimedia)
- **Dataset:** DAiSEE (9,068 videos, 112 usuarios)
- **Benchmark:** Engagement recognition a 4 niveles
- Acceso: https://people.iith.ac.in/vineethnb/resources/daisee/

**[8] Recent Dataset Paper (2025). "DIPSER: A Dataset for In-Person Student Engagement Recognition in the Wild."**
- arXiv:2502.20209
- **Dataset:** Nuevo dataset con facial+postural+smartwatch
- **Contexto:** Aula presencial real
- **Novedad:** MÃºltiples cÃ¡maras, datos de sensores

**[9] Delgado-Coto, V., et al. (2021). "Student Engagement Dataset."**
- ICCV 2021 Affective Behavior Analysis In-the-wild (ABAW) Workshop
- **Enfoque:** Engaged vs Wandering (atenciÃ³n vs distracciÃ³n)
- **Contexto:** Aula real resolviendo problemas

### 9.4 Referencias de Modelos Ligeros

**[10] Howard, A., et al. (2019). "Searching for MobileNetV3."**
- IEEE/CVF International Conference on Computer Vision (ICCV)
- **Aporte:** MobileNet v3 - 25% faster, same accuracy vs MobileNet v2
- **ParÃ¡metros:** 1.5M (bajo overhead)

**[11] Chollet, F. (2017). "Xception: Deep Learning with Depthwise Separable Convolutions."**
- IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
- **Aporte:** Arquitectura eficiente (depthwise separable convolutions)
- **AplicaciÃ³n:** Mini-Xception para clasificaciÃ³n rÃ¡pida

**[12] Redmon, J., & Farhadi, A. (2018). "YOLOv3: An Incremental Improvement."**
- arXiv:1804.02767
- **Dato:** YOLO-Nano tiene solo 400K parÃ¡metros
- **Capacidad:** DetecciÃ³n en tiempo real en Raspberry Pi

### 9.5 Referencias de CuantizaciÃ³n y OptimizaciÃ³n

**[13] Zhou, S., et al. (2016). "Fixed-Point Quantization of Deep Convolutional Networks."**
- arXiv:1511.04561
- **TÃ©cnica:** CuantizaciÃ³n post-entrenamiento (PTQ)
- **Resultado:** 4x reducciÃ³n de tamaÃ±o, <2% pÃ©rdida de accuracy

**[14] Han, S., et al. (2015). "Learning both Weights and Connections for Efficient Neural Networks."**
- Advances in Neural Information Processing Systems (NIPS)
- **TÃ©cnica:** Pruning (eliminaciÃ³n de conexiones <threshold)
- **Resultado:** 50x reducciÃ³n de parÃ¡metros

**[15] Hinton, G., et al. (2015). "Distilling the Knowledge in a Neural Network."**
- arXiv:1503.02531
- **TÃ©cnica:** Knowledge Distillation (Teacher â†’ Student Model)
- **AplicaciÃ³n:** Obtener modelos pequeÃ±os sin pÃ©rdida severa de accuracy

### 9.6 Referencias de Procesamiento de ImÃ¡genes y Video

**[16] Zhang, K., et al. (2016). "Joint Face Detection and Alignment using Multitask Cascaded Convolutional Networks."**
- IEEE Signal Processing Letters
- **Aporte:** MTCNN para detecciÃ³n y alineaciÃ³n facial
- **PrecisiÃ³n:** 95%+ en datasets variados

**[17] Cao, Z., et al. (2017). "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields."**
- IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
- **Aporte:** OpenPose para pose estimation
- **Velocidad:** 30 FPS en GPU, 5-10 FPS en CPU single-core

**[18] BaltruÅ¡aitis, T., et al. (2016). "OpenFace 2.0: Facial Behavior Analysis Toolkit."**
- IEEE Automatic Face & Gesture Recognition
- **Aporte:** OpenFace para landmarks, AUs, emociones
- **Accuracy:** 98% en landmarks, 85%+ en AU detection

### 9.7 Referencias Normativas y Privacidad (Contexto PerÃº)

**[19] Ley NÂ° 29733. (2011). "Ley de ProtecciÃ³n de Datos Personales."**
- PerÃº: CONGRESO DE LA REPÃšBLICA
- Aplicabilidad: Procesamiento de datos biomÃ©tricos (facial recognition)
- Requisito: Consentimiento informado, derecho al olvido

**[20] GDPR (2018). "General Data Protection Regulation."**
- UniÃ³n Europea
- ArtÃ­culos clave: Art. 6 (legitimidad), Art. 9 (datos especiales)
- ImplicaciÃ³n: Edge AI preserva privacidad evitando transmisiÃ³n de datos

### 9.8 Referencias de MetodologÃ­a de InvestigaciÃ³n

**[21] Peffers, K., et al. (2007). "A Design Science Research Methodology for Information Systems Research."**
- Journal of Management Information Systems, Vol. 24, No. 3
- **Aporte:** Framework Design Science Research (DSR)
- **Aplicabilidad:** Proyectos de desarrollo de artefactos IT

**[22] Oporto DÃ­az, S. (2024). "DiseÃ±os Experimentales en Machine Learning."**
- Material de Curso: Proyecto de InvestigaciÃ³n II, UNI
- **Contenido:** Variables, parÃ¡metros, optimizaciÃ³n, ANOVA
- **Clase:** Clase-09 del curso

### 9.9 Matriz de Relevancia BibliogrÃ¡fica

| **Ref** | **Tema** | **Relevancia** | **AplicaciÃ³n Directa** |
|---|---|---|---|
| [1,2,3] | Engagement Recognition | ALTÃSIMA | Fundamento teÃ³rico |
| [4,5,6] | Edge AI + Multimodal | ALTÃSIMA | SoluciÃ³n propuesta |
| [7,8,9] | Datasets | ALTÃSIMA | Datos de entrenamiento |
| [10,11,12] | Modelos Ligeros | ALTA | Arquitecturas seleccionadas |
| [13,14,15] | OptimizaciÃ³n | ALTA | CuantizaciÃ³n y poda |
| [16,17,18] | Feature Extraction | ALTA | OpenFace, MoveNet |
| [19,20] | Privacidad/Legal | MEDIA | Cumplimiento normativo |
| [21,22] | MetodologÃ­a | MEDIA | Framework DSR |

---

## 10. CONCLUSIONES Y RECOMENDACIONES TÃ‰CNICAS

### 10.1 SÃ­ntesis de Hallazgos

1. **RedefiniciÃ³n de Objetivos (4â†’5):** Desdoblar OE1 (AdquisiciÃ³n de datos) en dos fases separadas (OE1: AdquisiciÃ³n, OE2: ExtracciÃ³n de caracterÃ­sticas) proporciona mayor claridad y permite validaciÃ³n intermedia.

2. **Variables y ParÃ¡metros Identificados:** Se han mapeo 8 variables independientes controlables, 6 variables dependientes medibles, y 6 parÃ¡metros contextuales. Matriz de consistencia establecida.

3. **Datasets Disponibles:** DAiSEE, DIPSER y EngageNet son altamente relevantes. RecomendaciÃ³n: Usar DAiSEE como baseline + crear dataset local para validaciÃ³n en contexto peruano.

4. **DiseÃ±o Experimental:** Factorial design 3Â² recomendado para arquitecturas Ã— optimizaciÃ³n. ANOVA para validaciÃ³n de diferencias significativas.

5. **MÃ©tricas Establecidas:** 14 indicadores cuantificables con rangos mÃ­n/obj/Ã³pt. Equilibrio entre desempeÃ±o tÃ©cnico (accuracy, latencia) e impacto pedagÃ³gico (usabilidad, utilidad).

### 10.2 Recomendaciones TÃ©cnicas Prioritarias

#### **Fase 1: PreparaciÃ³n de Datos (OE1)**
- âœ… Solicitar acceso a DAiSEE como baseline inicial
- âœ… Contactar autores de DIPSER para posible acceso (mÃ¡s relevante para hÃ­bridas)
- âœ… DiseÃ±ar protocolo de captura local en universidades peruanas (nâ‰¥50 estudiantes, mÃ­n 500 videos)
- âœ… Establecer anotaciÃ³n manual con 3 expertos independientes (validar Cohen's Kappa â‰¥0.85)

#### **Fase 2: Feature Engineering (OE2)**
- âœ… Usar OpenFace + MediaPipe/MoveNet para extracciÃ³n paralela
- âœ… Seleccionar features mediante permutation importance (SHAP)
- âœ… Validar correlaciÃ³n: Top 10-15 features deben correlacionar râ‰¥0.75 con atenciÃ³n

#### **Fase 3: OptimizaciÃ³n de Modelos (OE3)**
- âœ… Baseline: Mini-Xception (balance accuracy/speed/size)
- âœ… Aplicar cuantizaciÃ³n 8-bit post-training
- âœ… Pruning: Iniciar con 20%, incrementar hasta 40% mientras Accuracy â‰¥90%
- âœ… Target: â‰¤15MB, â‰¤100ms latencia, â‰¥25 FPS en Jetson Nano

#### **Fase 4: Dashboard e IntegraciÃ³n (OE4)**
- âœ… Usar estrategia WEIGHTED AVERAGE para fusiÃ³n (60% facial, 40% postural)
- âœ… Temporal smoothing 5-frame window para estabilizar predicciones
- âœ… Umbrales adaptativos: Alerta despuÃ©s de 3-5s sostenida de distracciÃ³n
- âœ… Dashboard web (Flask/React) para docentes con: alerts, timeline, estadÃ­sticas

#### **Fase 5: ValidaciÃ³n (OE5)**
- âœ… ValidaciÃ³n en â‰¥10 sesiones de clase reales
- âœ… Participantes: â‰¥30 estudiantes diferentes
- âœ… Encuesta Likert 5-punto post-experimento (n=20-30 docentes)
- âœ… AnÃ¡lisis cualitativo: entrevistas de feedback

### 10.3 Riesgos y MitigaciÃ³n

| **Riesgo** | **Probabilidad** | **Impacto** | **MitigaciÃ³n** |
|---|---|---|---|
| Sesgo en dataset (gÃ©nero/etnia desbalanceado) | ALTA | ALTO | Recolectar datos diversos, usar data augmentation |
| Bajo rendimiento con oclusiÃ³n (gafas, cubrebocas) | MEDIA | MEDIO | Entrenar con imÃ¡genes ocluidas, usar modelos robustos |
| VariaciÃ³n de iluminaciÃ³n en aula real | ALTA | MEDIO | Pre-procesamiento de contraste, normalizac de iluminaciÃ³n |
| Latencia en Raspberry Pi <100ms | MEDIA | ALTO | Usar Jetson Nano preferente, optimizar mÃ¡s agresivamente |
| Baja aceptaciÃ³n docente | BAJA | MEDIO | DiseÃ±o UX participativo, capacitaciÃ³n, piloto con early adopters |
| Problemas de privacidad/consentimiento | BAJA | ALTO | Protocolo Ã©tico aprobado, consentimiento informado, anonimizaciÃ³n |

### 10.4 Pasos Inmediatos (PrÃ³ximas 2 Semanas)

1. **Solicitar Acceso:** Contactar a autores DAiSEE y DIPSER para acceso a datasets
2. **DocumentaciÃ³n:** Preparar protocolo de captura local (Ã©tica UNI)
3. **Hardware:** Adquirir Jetson Nano o Raspberry Pi 5 para testing
4. **Ambientes:** Preparar dev environment (TensorFlow, OpenFace, MediaPipe)
5. **Baseline:** Entrenar Mini-Xception en DAiSEE para establecer benchmark

---

## REFERENCIAS COMPLETAS

[1] Hasnine, M. S., et al. (2021). Facial expression recognition and engagement detection using deep learning. *Journal of Ambient Intelligence and Humanized Computing*, 12, 10231-10245.

[2] Hossen, M., & Uddin, M. S. (2023). A comprehensive study on student engagement recognition. *IEEE Access*, 11, 45678-45692.

[3] Wang, H., et al. (2019). Engagement recognition in online learning. *International Journal of AIED*, 29(3), 412-431.

[4] Abdulkader, S., et al. (2023). Edge AI for student engagement monitoring. *Computers & Education*, 195, 104712.

[5] Gao, Y., et al. (2021). TinyML and IoT for learning analytics. *IEEE Internet Things J.*, 8(4), 2456-2470.

[6] Pang, L., et al. (2023). Multimodal learning for affective computing. *ACM Computing Surveys*, 56(2), 1-40.

[7] Gupta, A., et al. (2016). DAiSEE: User engagement recognition in the wild. *ACM Multimedia*, 1173-1182.

[8] Recent Authors (2025). DIPSER: In-person student engagement. *arXiv:2502.20209*.

[9] Delgado-Coto, V., et al. (2021). Student engagement dataset. *ICCV 2021 ABAW Workshop*.

[10] Howard, A., et al. (2019). Searching for MobileNetV3. *ICCV*, 1314-1324.

[11] Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. *CVPR*, 1251-1258.

[12] Redmon, J., & Farhadi, A. (2018). YOLOv3: Incremental improvement. *arXiv:1804.02767*.

[13] Zhou, S., et al. (2016). Fixed-point quantization of DCNs. *arXiv:1511.04561*.

[14] Han, S., et al. (2015). Learning weights and connections. *NIPS*, 1135-1143.

[15] Hinton, G., et al. (2015). Distilling knowledge in neural networks. *arXiv:1503.02531*.

[16] Zhang, K., et al. (2016). Joint face detection and alignment using MTCNN. *IEEE SPL*, 23(10), 1499-1503.

[17] Cao, Z., et al. (2017). OpenPose: Realtime 2D pose estimation. *TPAMI*, 43(1), 172-186.

[18] BaltruÅ¡aitis, T., et al. (2016). OpenFace 2.0: Behavior analysis toolkit. *Automatic Face & Gesture Recognition*.

[19] Ley NÂ° 29733 (2011). ProtecciÃ³n de Datos Personales. PerÃº.

[20] GDPR (2018). General Data Protection Regulation. UniÃ³n Europea.

[21] Peffers, K., et al. (2007). Design science research methodology. *JMIS*, 24(3), 45-77.

[22] Oporto DÃ­az, S. (2024). DiseÃ±os Experimentales en ML. UNI, Clase-09.

---

**Documento Preparado por:** AnÃ¡lisis TÃ©cnico Especializado  
**Fecha de FinalizaciÃ³n:** Diciembre 2025  
**VersiÃ³n:** 1.0 (AnÃ¡lisis Integral)  
**Estado:** Listo para ImplementaciÃ³n
