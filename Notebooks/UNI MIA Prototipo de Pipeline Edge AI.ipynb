{"cells":[{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","import mediapipe as mp\n","import time\n","import math\n","\n","# --- 1. Inicialización de Componentes ---\n","\n","# Inicializar MediaPipe Pose\n","mp_pose = mp.solutions.pose\n","pose = mp_pose.Pose(static_image_mode=False,\n","                    model_complexity=0,  # 0: Lite, 1: Full, 2: Heavy\n","                    min_detection_confidence=0.5,\n","                    min_tracking_confidence=0.5)\n","mp_drawing = mp.solutions.drawing_utils\n","\n","# --- 2. Simulación de Carga de Modelo TFLite (OE 2) ---\n","\n","def load_tflite_model(model_path):\n","    \"\"\"\n","    Carga un modelo TFLite y asigna los tensores.\n","    Simula la carga del modelo facial optimizado (INT8).\n","    \"\"\"\n","    try:\n","        # Cargar el intérprete TFLite\n","        interpreter = tf.lite.Interpreter(model_path=model_path)\n","        interpreter.allocate_tensors()\n","\n","        # Obtener detalles de entrada y salida\n","        input_details = interpreter.get_input_details()\n","        output_details = interpreter.get_output_details()\n","\n","        print(f\"Modelo TFLite cargado: {model_path}\")\n","        print(f\"Detalles de Entrada: {input_details[0]['shape']}, Tipo: {input_details[0]['dtype']}\")\n","        print(f\"Detalles de Salida: {output_details[0]['shape']}, Tipo: {output_details[0]['dtype']}\")\n","\n","        return interpreter, input_details, output_details\n","\n","    except ValueError:\n","        print(f\"Error: No se pudo cargar el modelo TFLite de {model_path}.\")\n","        print(\"Asegúrese de que el archivo .tflite exista.\")\n","        print(\"Generando un intérprete simulado para continuar...\")\n","        # Fallback: Crear un intérprete simulado si el archivo no existe\n","        return None, None, None\n","    except Exception as e:\n","        print(f\"Error inesperado al cargar {model_path}: {e}\")\n","        return None, None, None\n","\n","# --- 3. Clases de Procesamiento Multimodal (OE 1 y 3) ---\n","\n","class MultimodalAttentionPipeline:\n","\n","    def __init__(self, facial_model_path):\n","        # Cargar el modelo facial (emociones)\n","        # Usaremos un placeholder para las clases de emoción\n","        self.emotion_labels = ['Confusión', 'Fatiga/Aburrimiento', 'Interés/Atención']\n","\n","        # (Simulación) Cargar el modelo TFLite de emoción (ej. Mini-Xception INT8)\n","        # self.facial_interpreter, self.facial_input, self.facial_output = load_tflite_model(facial_model_path)\n","\n","        # Placeholder si el modelo TFLite no se carga\n","        print(\"ADVERTENCIA: Modelo facial TFLite no cargado. Usando predicciones simuladas.\")\n","        self.facial_interpreter = None\n","\n","        # Cargar el modelo postural (MediaPipe)\n","        self.pose_estimator = mp_pose.Pose(model_complexity=0, min_detection_confidence=0.5)\n","\n","    def _calculate_angle(self, a, b, c):\n","        \"\"\"Calcula el ángulo entre tres puntos (landmarks).\"\"\"\n","        a = np.array(a) # Primer punto\n","        b = np.array(b) # Punto medio (vértice)\n","        c = np.array(c) # Tercer punto\n","\n","        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n","        angle = np.abs(radians * 180.0 / np.pi)\n","\n","        if angle > 180.0:\n","            angle = 360 - angle\n","        return angle\n","\n","    def process_facial_emotion(self, face_roi):\n","        \"\"\"\n","        Procesa el ROI facial para la detección de emociones.\n","        (Simulación de inferencia TFLite)\n","        \"\"\"\n","        if self.facial_interpreter is None:\n","            # Simulación si el modelo no está cargado\n","            # Devuelve una distribución de probabilidad aleatoria\n","            simulated_preds = np.random.rand(len(self.emotion_labels))\n","            simulated_preds /= np.sum(simulated_preds)\n","            return simulated_preds\n","\n","        # Proceso real de TFLite (requiere el modelo)\n","        # 1. Pre-procesar ROI (ej. 64x64, escala de grises, normalizar)\n","        # input_data = cv2.resize(face_roi, (64, 64))\n","        # input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2GRAY)\n","        # input_data = np.expand_dims(input_data, axis=-1)\n","        # input_data = np.expand_dims(input_data, axis=0)\n","        # input_data = input_data.astype(np.float32) / 255.0\n","\n","        # 2. Establecer tensor de entrada\n","        # self.facial_interpreter.set_tensor(self.facial_input[0]['index'], input_data)\n","\n","        # 3. Invocar inferencia\n","        # self.facial_interpreter.invoke()\n","\n","        # 4. Obtener salida\n","        # preds = self.facial_interpreter.get_tensor(self.facial_output[0]['index'])[0]\n","        # return preds\n","\n","        pass # Reemplazado por la simulación de arriba\n","\n","    def process_pose_estimation(self, frame_rgb):\n","        \"\"\"\n","        Procesa el frame completo para la estimación de pose con MediaPipe.\n","        \"\"\"\n","        results = self.pose_estimator.process(frame_rgb)\n","        features = {'head_angle': 0.0, 'is_slouching': False}\n","\n","        if results.pose_landmarks:\n","            landmarks = results.pose_landmarks.landmark\n","\n","            # Obtener landmarks clave (nariz, hombros)\n","            try:\n","                nose = [landmarks[mp_pose.PoseLandmark.NOSE.value].x, landmarks[mp_pose.PoseLandmark.NOSE.value].y]\n","                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n","                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n","\n","                # Calcular punto medio de hombros\n","                shoulder_mid = [(left_shoulder[0] + right_shoulder[0]) / 2, (left_shoulder[1] + right_shoulder[1]) / 2]\n","\n","                # Calcular ángulo de inclinación de la cabeza (vertical)\n","                # (Usamos un punto artificial 'arriba' de la nariz para medir la verticalidad)\n","                nose_up = [nose[0], nose[1] - 0.1]\n","\n","                # Ángulo: (nose_up) - (nose) - (shoulder_mid)\n","                angle = self._calculate_angle(nose_up, nose, shoulder_mid)\n","                features['head_angle'] = angle\n","\n","                # Detectar encorvamiento (slouching)\n","                if nose[1] > shoulder_mid[1]: # Si la nariz está por debajo de los hombros\n","                    features['is_slouching'] = True\n","\n","            except Exception as e:\n","                pass # Error al obtener landmarks\n","\n","        return features, results.pose_landmarks\n","\n","    def fuse_and_classify(self, facial_preds, pose_features):\n","        \"\"\"\n","        Motor de Fusión Multimodal (Lógica de OE 3).\n","        Combina las salidas para determinar el estado afectivo.\n","        \"\"\"\n","\n","        # 1. Interpretar predicciones faciales\n","        emotion_idx = np.argmax(facial_preds)\n","        emotion_label = self.emotion_labels[emotion_idx]\n","        emotion_conf = facial_preds[emotion_idx]\n","\n","        # 2. Aplicar reglas de fusión\n","\n","        # Regla 1: Postura domina para Fatiga\n","        # (Ángulo > 30 grados indica cabeza muy inclinada)\n","        if pose_features['is_slouching'] or pose_features['head_angle'] > 30:\n","            return \"Fatiga\", (0, 0, 255) # Rojo\n","\n","        # Regla 2: Emoción facial domina para Aburrimiento/Confusión\n","        if emotion_label == 'Fatiga/Aburrimiento' and emotion_conf > 0.6:\n","            return \"Distracción (Aburrido)\", (0, 165, 255) # Naranja\n","\n","        if emotion_label == 'Confusión' and emotion_conf > 0.5:\n","            return \"Confusión (Posible Duda)\", (0, 255, 255) # Amarillo\n","\n","        # Regla 3: Default es Atención\n","        return \"Atención\", (0, 255, 0) # Verde\n","\n","# --- 4. Bucle Principal de Ejecución (Demo del Prototipo) ---\n","\n","if __name__ == \"__main__\":\n","\n","    # (Simulación OE 2) Ruta al modelo facial optimizado\n","    # Descargar un modelo de emoción TFLite (ej. de Kaggle o TF Hub)\n","    # y renombrarlo a 'emotion_model.tflite'\n","    FACIAL_MODEL_PATH = \"emotion_model.tflite\"\n","\n","    pipeline = MultimodalAttentionPipeline(FACIAL_MODEL_PATH)\n","\n","    # Iniciar captura de video (Cámara 0 o un archivo de video)\n","    # cap = cv2.VideoCapture(0)\n","    cap = cv2.VideoCapture(\"ruta_a_un_video_de_aula_virtual.mp4\") # O usar un archivo\n","\n","    if not cap.isOpened():\n","        print(\"Error: No se pudo abrir la fuente de video.\")\n","        exit()\n","\n","    # (Simulación OE 1) Usamos un detector de rostros simple (Haar Cascade)\n","    # para encontrar el ROI, ya que el modelo de emoción espera un rostro.\n","    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","    print(\"Iniciando pipeline... Presione 'q' para salir.\")\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        start_time = time.time() # Iniciar contador para FPS\n","\n","        # --- Pipeline de Inferencia ---\n","\n","        # 1. Convertir a RGB (requerido por MediaPipe)\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        # 2. Procesamiento Postural (OE 1)\n","        pose_features, landmarks = pipeline.process_pose_estimation(frame_rgb)\n","\n","        # 3. Procesamiento Facial (OE 1)\n","        # Encontrar el rostro (ROI)\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n","\n","        facial_preds = np.array([0, 0, 1]) # Default: Atención\n","\n","        if len(faces) > 0:\n","            # Usar solo el primer rostro detectado\n","            (x, y, w, h) = faces[0]\n","            face_roi = frame_rgb[y:y+h, x:x+w]\n","\n","            # Inferencia del modelo de emoción (OE 2)\n","            facial_preds = pipeline.process_facial_emotion(face_roi)\n","\n","            # Dibujar ROI facial\n","            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n","\n","        # 4. Fusión Multimodal (OE 3)\n","        estado_afectivo, color = pipeline.fuse_and_classify(facial_preds, pose_features)\n","\n","        # --- Cálculo de Métricas (OE 4) ---\n","        end_time = time.time()\n","        latency = (end_time - start_time) * 1000 # Latencia en ms\n","        fps = 1.0 / (end_time - start_time) if (end_time - start_time) > 0 else 0\n","\n","        # --- Visualización (Simulación Dashboard OE 3) ---\n","\n","        # Dibujar landmarks posturales\n","        if landmarks:\n","            mp_drawing.draw_landmarks(\n","                frame, landmarks, mp_pose.POSE_CONNECTIONS)\n","\n","        # Mostrar Estado Afectivo\n","        cv2.putText(frame, f\"Estado: {estado_afectivo}\", (10, 30),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n","\n","        # Mostrar Métricas de Rendimiento (OE 4)\n","        cv2.putText(frame, f\"Latencia: {latency:.2f} ms\", (10, 60),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n","        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 90),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n","\n","        # (Simulación Dashboard)\n","        # Aquí es donde se enviarían los datos (estado_afectivo, latency)\n","        # a un servidor Flask/Streamlit para el dashboard del docente.\n","\n","        cv2.imshow('Prototipo Edge AI - Aula Híbrida', frame)\n","\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()"],"outputs":[],"execution_count":null,"metadata":{"id":"20LqXu4weeHq"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}