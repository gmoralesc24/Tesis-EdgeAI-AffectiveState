{"cells":[{"cell_type":"code","source":["# =============================================================================\n","# NOTAS INICIALES: Flujo de trabajo para el preprocesamiento de los datos\n","# visuales (imágenes y metadata) del dataset.\n","# =============================================================================\n","\n","\n","# # Proyecto: Medición de Estados Afectivos en Aulas Híbridas\n","# ## Módulo 1: Preprocesamiento y Extracción de Características Visuales\n","#\n","# Este módulo se centra en la ingesta, limpieza y preparación de los datos\n","# provenientes de las carpetas `images` y `metadata`, utilizando las etiquetas\n","# de la carpeta `labels` como Ground Truth (verdad fundamental).\n","#\n","# **Objetivos**\n","# 1.  Cargar y sincronizar los archivos de imagen (`.jpg`) con sus archivos de metadata (`.json`).\n","# 2.  Utilizar las coordenadas de detección facial de la metadata para realizar un recorte (crop) en las imágenes.\n","# 3.  Integrar las etiquetas humanas (inter-rater agreement) para crear el Ground Truth final.\n","# 4.  Generar un DataFrame unificado listo para el entrenamiento del modelo.\n","\n","\n","import os\n","import json\n","import pandas as pd\n","import cv2  # OpenCV para manipulación de imágenes\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from glob import glob\n","from collections import defaultdict\n","from tqdm import tqdm\n","\n","\n","# =============================================================================\n","# 1. Configuración de Rutas y Constantes\n","# =============================================================================\n","# NOTA: Se asume que este script se ejecuta desde un directorio que contiene\n","# las carpetas 'images', 'labels' y 'metadata'.\n","\n","BASE_DIR = os.getcwd() # Directorio actual\n","IMAGES_DIR = os.path.join(BASE_DIR, 'images')\n","METADATA_DIR = os.path.join(BASE_DIR, 'metadata')\n","LABELS_DIR = os.path.join(BASE_DIR, 'labels')\n","\n","# Definiciones de etiquetas (adaptar según la Escala Likert o las categorías usadas)\n","# Ejemplo basado en el documento:\n","ATENTION_MAPPING = {\n","    '1': 'Baja', '2': 'Media', '3': 'Alta' # Ejemplo de una escala de 3 puntos\n","}\n","EMOTION_MAPPING = {\n","    '1': 'Neutral', '2': 'Felicidad', '3': 'Tristeza', '4': 'Sorpresa',\n","    '5': 'Ira', '6': 'Disgusto', '7': 'Miedo', '8': 'Desinterés' # Ejemplo de 8 categorías\n","}\n","\n","# Clases objetivo del modelo (Ground Truth)\n","TARGET_ATTENTION_COL = 'attention_consensus'\n","TARGET_EMOTION_COL = 'emotion_consensus'\n","\n","\n","# =============================================================================\n","# 2. Funciones de Carga y Preprocesamiento de Etiquetas Humanas (Ground Truth)\n","# =============================================================================\n","\n","def load_and_merge_labels(labels_dir):\n","    \"\"\"\n","    Carga todos los archivos JSON de la carpeta 'labels', los fusiona y\n","    calcula un consenso de etiquetas (Ground Truth).\n","    \"\"\"\n","    all_labels = []\n","    label_files = glob(os.path.join(labels_dir, 'labeler_*.json'))\n","    # También incluimos la auto-etiquetación si existe (ej. self_labeling.json)\n","    self_label_file = os.path.join(labels_dir, 'self_labeling.json')\n","    if os.path.exists(self_label_file):\n","        label_files.append(self_label_file)\n","\n","    print(f\"Archivos de etiquetas encontrados: {len(label_files)}\")\n","\n","    for filepath in label_files:\n","        labeler_id = os.path.basename(filepath).split('.')[0]\n","        try:\n","            with open(filepath, 'r') as f:\n","                data = json.load(f)\n","                df_temp = pd.DataFrame(data)\n","                df_temp['labeler_id'] = labeler_id\n","                all_labels.append(df_temp)\n","        except Exception as e:\n","            print(f\"Error al cargar {filepath}: {e}\")\n","\n","    if not all_labels:\n","        print(\"No se cargaron archivos de etiquetas.\")\n","        return pd.DataFrame()\n","\n","    df_raw = pd.concat(all_labels, ignore_index=True)\n","\n","    # Convertir a numérico y manejar errores\n","    df_raw['attention'] = pd.to_numeric(df_raw['attention'], errors='coerce')\n","    df_raw['emotion'] = pd.to_numeric(df_raw['emotion'], errors='coerce')\n","\n","    # Convertir 'datetime' a formato consistente (analizar formato original primero)\n","    # Corrected format string to match the colon separator for microseconds\n","    df_raw['datetime'] = pd.to_datetime(df_raw['datetime'], format='%H:%M:%S:%f', errors='coerce').dt.strftime('%H_%M_%S_%f')\n","\n","\n","    # Calcular el consenso: se utiliza la moda (el valor más votado)\n","    consensus_df = df_raw.groupby('datetime').agg(\n","        attention_consensus=('attention', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n","        emotion_consensus=('emotion', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n","        n_labels_attention=('attention', 'count'),\n","        n_labels_emotion=('emotion', 'count')\n","    ).reset_index()\n","\n","    # Eliminar NaNs generados por filas vacías o datos inválidos\n","    consensus_df.dropna(subset=[TARGET_ATTENTION_COL, TARGET_EMOTION_COL], how='all', inplace=True)\n","\n","    print(f\"Etiquetas de consenso generadas para {len(consensus_df)} momentos.\")\n","    return consensus_df\n","\n","\n","# =============================================================================\n","# 3. Funciones de Carga y Preprocesamiento de Metadata\n","# =============================================================================\n","\n","def load_and_flatten_metadata(metadata_dir):\n","    \"\"\"\n","    Carga todos los archivos JSON de la carpeta 'metadata', extrae las\n","    características de bajo nivel y las consolida en un DataFrame.\n","    \"\"\"\n","    metadata_list = []\n","    metadata_files = glob(os.path.join(metadata_dir, '*.json'))\n","\n","    print(f\"Archivos de metadata encontrados: {len(metadata_files)}\")\n","\n","    for filepath in tqdm(metadata_files, desc=\"Procesando metadata\"):\n","        filename = os.path.basename(filepath)\n","        datetime_id = filename.split('.')[0] # Asumiendo que el nombre es la marca de tiempo\n","\n","        try:\n","            with open(filepath, 'r') as f:\n","                data = json.load(f)\n","                row = {'datetime': datetime_id, 'metadata_file': filename}\n","\n","                # 1. Extracción de Bounding Box y Características Demográficas\n","                face_data = data.get('person', {}).get('face', {})\n","                if not face_data:\n","                    continue # Saltar si no hay detección de rostro\n","\n","                # Bounding Box\n","                bbox = face_data.get('bounding_box', {})\n","                row.update({f'bbox_{k}': v for k, v in bbox.items()})\n","\n","                # Demográficos\n","                row['age'] = face_data.get('age')\n","                row['gender_name'] = face_data.get('gender', {}).get('gender_name')\n","\n","                # 2. Extracción de Probabilidades de Emoción (Multimodal - Facial)\n","                emotion_probs = face_data.get('emotion', {}).get('probability_emotion', {})\n","                row.update({f'prob_emotion_{k}': v for k, v in emotion_probs.items()})\n","                row['dominant_emotion'] = face_data.get('emotion', {}).get('dominant_emotion')\n","\n","                # 3. Extracción de Features de Postura (Multimodal - Postural)\n","                # Se pueden extraer características resumidas de los 'landmarks'\n","                # Por ejemplo, la visibilidad promedio de los landmarks del cuerpo\n","                landmarks = data.get('person', {}).get('landmarks', [])\n","                visibility = [l.get('visibility', 0) for l in landmarks]\n","                presence = [l.get('presence', 0) for l in landmarks]\n","\n","                row['avg_landmark_visibility'] = np.mean(visibility) if visibility else 0\n","                row['avg_landmark_presence'] = np.mean(presence) if presence else 0\n","\n","                metadata_list.append(row)\n","\n","        except Exception as e:\n","            print(f\"Error al procesar metadata en {filename}: {e}\")\n","\n","    df_metadata = pd.DataFrame(metadata_list)\n","    # Store the original datetime from the filename for image processing\n","    df_metadata['original_datetime'] = df_metadata['datetime']\n","    # Convert datetime to the same format as in df_labels for merging\n","    df_metadata['datetime'] = pd.to_datetime(df_metadata['datetime'], format='%H_%M_%S_%f', errors='coerce').dt.strftime('%H_%M_%S_%f')\n","\n","    print(f\"Metadata consolidada para {len(df_metadata)} registros.\")\n","    return df_metadata\n","\n","\n","# =============================================================================\n","# 4. Función de Preprocesamiento de Imágenes (Recorte Facial)\n","# =============================================================================\n","\n","def crop_and_save_face(row, output_dir='processed_faces', margin_ratio=0.3):\n","    \"\"\"\n","    Carga la imagen, utiliza el Bounding Box de la metadata y recorta el rostro,\n","    guardando el resultado en una nueva carpeta.\n","    \"\"\"\n","    # Use the original datetime from metadata for image filename\n","    img_filename = f\"{row['original_datetime']}.jpg\"\n","    img_path = os.path.join(IMAGES_DIR, img_filename)\n","    output_path = os.path.join(output_dir, img_filename)\n","\n","    # Crear directorio de salida si no existe\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    if not os.path.exists(img_path):\n","        # print(f\"Imagen no encontrada: {img_path}\")\n","        return None # Devuelve None si la imagen no existe\n","\n","    try:\n","        # Cargar imagen\n","        image = cv2.imread(img_path)\n","        if image is None:\n","            raise FileNotFoundError(\"Error al cargar la imagen con OpenCV\")\n","\n","        H, W = image.shape[:2]\n","\n","        # Extraer coordenadas del Bounding Box\n","        x0 = int(row['bbox_x0'])\n","        y0 = int(row['bbox_y0'])\n","        x1 = int(row['bbox_x1'])\n","        y1 = int(row['bbox_y1'])\n","\n","        # Calcular el ancho y alto originales del BBox\n","        w_orig = x1 - x0\n","        h_orig = y1 - y0\n","\n","        # Añadir un margen (30% del tamaño original) para contexto\n","        margin_w = int(w_orig * margin_ratio)\n","        margin_h = int(h_orig * margin_ratio)\n","\n","        # Recalcular las coordenadas con margen, asegurando que no se salgan de la imagen\n","        x_min = max(0, x0 - margin_w)\n","        y_min = max(0, y0 - margin_h)\n","        x_max = min(W, x1 + margin_w)\n","        y_max = min(H, y1 + margin_h)\n","\n","        # Recortar la región del rostro con margen\n","        cropped_face = image[y_min:y_max, x_min:x_max]\n","\n","        # Guardar la imagen recortada\n","        cv2.imwrite(output_path, cropped_face)\n","\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error procesando imagen {img_filename}: {e}\")\n","        return None\n","\n","def visualize_sample_image(df, n_samples=3, processed_dir='processed_faces'):\n","    \"\"\"\n","    Muestra una comparación visual de la imagen original vs. la imagen recortada.\n","    \"\"\"\n","    if df.empty:\n","        print(\"DataFrame vacío, no se puede visualizar.\")\n","        return\n","\n","    sample = df.sample(min(n_samples, len(df)))\n","\n","    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 5 * n_samples))\n","\n","    if n_samples == 1: # Para manejar el caso de una sola muestra\n","        axes = np.expand_dims(axes, axis=0)\n","\n","    for i, (_, row) in enumerate(sample.iterrows()):\n","        datetime_id = row['datetime']\n","        # Use the original datetime from metadata for image filename\n","        original_img_path = os.path.join(IMAGES_DIR, f\"{row['original_datetime']}.jpg\")\n","        processed_img_path = os.path.join(processed_dir, f\"{row['original_datetime']}.jpg\")\n","\n","        # Imagen Original\n","        if os.path.exists(original_img_path):\n","            img_orig = cv2.cvtColor(cv2.imread(original_img_path), cv2.COLOR_BGR2RGB)\n","            axes[i, 0].imshow(img_orig)\n","            axes[i, 0].set_title(f\"Original: {datetime_id}\")\n","            axes[i, 0].axis('off')\n","\n","        # Imagen Recortada\n","        if os.path.exists(processed_img_path):\n","            img_proc = cv2.cvtColor(cv2.imread(processed_img_path), cv2.COLOR_BGR2RGB)\n","            axes[i, 1].imshow(img_proc)\n","            attention = ATENTION_MAPPING.get(str(int(row[TARGET_ATTENTION_COL])), 'N/A')\n","            emotion = EMOTION_MAPPING.get(str(int(row[TARGET_EMOTION_COL])), 'N/A')\n","            axes[i, 1].set_title(f\"Recorte (GT: Att={attention}, Emo={emotion})\")\n","            axes[i, 1].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","# =============================================================================\n","# 5. Pipeline Principal de Ejecución\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    print(\"--- 1. Carga y Consenso de Etiquetas Humanas (Ground Truth) ---\")\n","    df_labels = load_and_merge_labels(LABELS_DIR)\n","\n","    if df_labels.empty:\n","        print(\"Proceso terminado: No se pudo generar el DataFrame de etiquetas.\")\n","        exit()\n","\n","    print(\"\\n--- 2. Carga y Consolidación de Metadata (Características) ---\")\n","    df_metadata = load_and_flatten_metadata(METADATA_DIR)\n","\n","    if df_metadata.empty:\n","        print(\"Proceso terminado: No se pudo generar el DataFrame de metadata.\")\n","        exit()\n","\n","    print(\"\\n--- 3. Fusión de Datos (Labels + Metadata) ---\")\n","    # Realizar el merge por el identificador de tiempo 'datetime'\n","    df_final = pd.merge(df_metadata, df_labels, on='datetime', how='inner')\n","\n","    print(f\"DataFrame Final Unificado (Registros listos para entrenamiento): {len(df_final)}\")\n","    print(df_final.head())\n","\n","    # --- 4. Preprocesamiento de Imágenes ---\n","    PROCESSED_FACES_DIR = os.path.join(BASE_DIR, 'processed_faces')\n","    print(f\"\\n--- 4. Procesamiento Visual (Recorte Facial) en: {PROCESSED_FACES_DIR} ---\")\n","\n","    # Add inspection of image filenames\n","    print(\"\\n--- Inspecting image filenames ---\")\n","    image_files = glob(os.path.join(IMAGES_DIR, '*.jpg'))\n","    if image_files:\n","        print(\"Sample image filenames:\")\n","        for i, filename in enumerate(image_files):\n","            if i < 5:\n","                print(os.path.basename(filename))\n","            else:\n","                break\n","    else:\n","        print(\"No image files found in IMAGES_DIR.\")\n","    print(\"--- End Inspection ---\")\n","\n","\n","    # Aplicar la función de recorte a cada fila del DataFrame final\n","    tqdm.pandas(desc=\"Recortando y guardando rostros\")\n","    df_final['processed_img_path'] = df_final.progress_apply(crop_and_save_face, axis=1, output_dir=PROCESSED_FACES_DIR)\n","\n","    print(\"\\n--- Inspecting processed_img_path before dropping NaNs ---\")\n","    display(df_final['processed_img_path'])\n","    print(\"--- End Inspection ---\")\n","\n","    # Limpiar registros donde la imagen no pudo ser procesada o no existe\n","    df_final.dropna(subset=['processed_img_path'], inplace=True)\n","\n","    print(f\"\\nDataFrame Final después del procesamiento de imágenes: {len(df_final)}\")\n","\n","    # --- 5. Exportación del DataFrame Final ---\n","    FINAL_CSV_PATH = os.path.join(BASE_DIR, 'processed_multimodal_data.csv')\n","    df_final.to_csv(FINAL_CSV_PATH, index=False)\n","    print(f\"\\nDatos consolidados exportados a: {FINAL_CSV_PATH}\")\n","\n","    # --- 6. Visualización de Muestra ---\n","    print(\"\\n--- 6. Visualización de Muestras (Original vs. Recortada) ---\")\n","    # Requiere tener las librerías cv2 y matplotlib instaladas.\n","    # visualize_sample_image(df_final, n_samples=3, processed_dir=PROCESSED_FACES_DIR)\n","\n","    print(\"\\nProceso de preprocesamiento visual completado.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. Carga y Consenso de Etiquetas Humanas (Ground Truth) ---\n","Archivos de etiquetas encontrados: 5\n","Etiquetas de consenso generadas para 30 momentos.\n","\n","--- 2. Carga y Consolidación de Metadata (Características) ---\n","Archivos de metadata encontrados: 2137\n"]},{"output_type":"stream","name":"stderr","text":["Procesando metadata: 100%|██████████| 2137/2137 [00:03<00:00, 634.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Metadata consolidada para 2137 registros.\n","\n","--- 3. Fusión de Datos (Labels + Metadata) ---\n","DataFrame Final Unificado (Registros listos para entrenamiento): 30\n","          datetime         metadata_file  bbox_x0  bbox_y0  bbox_x1  bbox_y1  \\\n","0  10_43_29_013977  10_43_29_013977.json      244       94      449      326   \n","1  10_43_53_037660  10_43_53_037660.json      259       85      422      270   \n","2  10_42_49_023396  10_42_49_023396.json      253       56      427      262   \n","3  10_43_05_018277  10_43_05_018277.json      240       85      442      320   \n","4  10_45_38_029444  10_45_38_029444.json      255      101      410      277   \n","\n","     age gender_name  prob_emotion_angry  prob_emotion_disgust  ...  \\\n","0  30.95        male           91.813862              0.586459  ...   \n","1  29.82        male           79.866221              0.001679  ...   \n","2  29.11        male           15.388538              6.557634  ...   \n","3  31.37        male           45.467147             40.156412  ...   \n","4  28.86        male            7.530159              0.007841  ...   \n","\n","   prob_emotion_surprise  prob_emotion_neutral  dominant_emotion  \\\n","0               0.012235              1.729295             angry   \n","1               0.000417              9.265746             angry   \n","2               0.036593             23.057017               sad   \n","3               0.026563              2.458548             angry   \n","4               0.013823             48.205975           neutral   \n","\n","   avg_landmark_visibility  avg_landmark_presence original_datetime  \\\n","0                        0                      0   10_43_29_013977   \n","1                        0                      0   10_43_53_037660   \n","2                        0                      0   10_42_49_023396   \n","3                        0                      0   10_43_05_018277   \n","4                        0                      0   10_45_38_029444   \n","\n","   attention_consensus  emotion_consensus n_labels_attention  n_labels_emotion  \n","0                  NaN                1.0                  0                 1  \n","1                  3.0                NaN                  1                 0  \n","2                  2.0                NaN                  1                 0  \n","3                  NaN                8.0                  0                 1  \n","4                  2.0                NaN                  1                 0  \n","\n","[5 rows x 23 columns]\n","\n","--- 4. Procesamiento Visual (Recorte Facial) en: /content/processed_faces ---\n","\n","--- Inspecting image filenames ---\n","No image files found in IMAGES_DIR.\n","--- End Inspection ---\n"]},{"output_type":"stream","name":"stderr","text":["Recortando y guardando rostros: 100%|██████████| 30/30 [00:00<00:00, 10485.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Inspecting processed_img_path before dropping NaNs ---\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["0     None\n","1     None\n","2     None\n","3     None\n","4     None\n","5     None\n","6     None\n","7     None\n","8     None\n","9     None\n","10    None\n","11    None\n","12    None\n","13    None\n","14    None\n","15    None\n","16    None\n","17    None\n","18    None\n","19    None\n","20    None\n","21    None\n","22    None\n","23    None\n","24    None\n","25    None\n","26    None\n","27    None\n","28    None\n","29    None\n","Name: processed_img_path, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>processed_img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--- End Inspection ---\n","\n","DataFrame Final después del procesamiento de imágenes: 0\n","\n","Datos consolidados exportados a: /content/processed_multimodal_data.csv\n","\n","--- 6. Visualización de Muestras (Original vs. Recortada) ---\n","\n","Proceso de preprocesamiento visual completado.\n"]}],"execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"af6jJgFDhMTe","executionInfo":{"status":"ok","timestamp":1761535042721,"user_tz":300,"elapsed":3469,"user":{"displayName":"Geyson David Morales Ccasa","userId":"04568127171487114327"}},"outputId":"3d04c0e4-27bb-492f-b02d-7ad52005066a"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}