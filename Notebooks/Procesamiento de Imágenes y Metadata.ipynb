{"cells":[{"cell_type":"code","source":"# =============================================================================\n# NOTAS INICIALES: Este script simula un Júpiter Notebook o entorno similar,\n# estructurando el flujo de trabajo para el preprocesamiento de los datos\n# visuales (imágenes y metadata) del dataset.\n# =============================================================================\n\n# %% [markdown]\n# # Proyecto: Medición de Estados Afectivos en Aulas Híbridas\n# ## Módulo 1: Preprocesamiento y Extracción de Características Visuales\n#\n# Este módulo se centra en la ingesta, limpieza y preparación de los datos\n# provenientes de las carpetas `images` y `metadata`, utilizando las etiquetas\n# de la carpeta `labels` como Ground Truth (verdad fundamental).\n#\n# **Objetivos del Notebook:**\n# 1.  Cargar y sincronizar los archivos de imagen (`.jpg`) con sus archivos de metadata (`.json`).\n# 2.  Utilizar las coordenadas de detección facial de la metadata para realizar un recorte (crop) en las imágenes.\n# 3.  Integrar las etiquetas humanas (inter-rater agreement) para crear el Ground Truth final.\n# 4.  Generar un DataFrame unificado listo para el entrenamiento del modelo.\n\n# %% [imports]\nimport os\nimport json\nimport pandas as pd\nimport cv2  # OpenCV para manipulación de imágenes\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\n# %% [setup]\n# =============================================================================\n# 1. Configuración de Rutas y Constantes\n# =============================================================================\n# NOTA: Se asume que este script se ejecuta desde un directorio que contiene\n# las carpetas 'images', 'labels' y 'metadata'.\n\nBASE_DIR = os.getcwd() # Directorio actual\nIMAGES_DIR = os.path.join(BASE_DIR, 'images')\nMETADATA_DIR = os.path.join(BASE_DIR, 'metadata')\nLABELS_DIR = os.path.join(BASE_DIR, 'labels')\n\n# Definiciones de etiquetas (adaptar según la Escala Likert o las categorías usadas)\n# Ejemplo basado en el documento:\nATENTION_MAPPING = {\n    '1': 'Baja', '2': 'Media', '3': 'Alta' # Ejemplo de una escala de 3 puntos\n}\nEMOTION_MAPPING = {\n    '1': 'Neutral', '2': 'Felicidad', '3': 'Tristeza', '4': 'Sorpresa',\n    '5': 'Ira', '6': 'Disgusto', '7': 'Miedo', '8': 'Desinterés' # Ejemplo de 8 categorías\n}\n\n# Clases objetivo del modelo (Ground Truth)\nTARGET_ATTENTION_COL = 'attention_consensus'\nTARGET_EMOTION_COL = 'emotion_consensus'\n\n# %% [section]\n# =============================================================================\n# 2. Funciones de Carga y Preprocesamiento de Etiquetas Humanas (Ground Truth)\n# =============================================================================\n\ndef load_and_merge_labels(labels_dir):\n    \"\"\"\n    Carga todos los archivos JSON de la carpeta 'labels', los fusiona y\n    calcula un consenso de etiquetas (Ground Truth).\n    \"\"\"\n    all_labels = []\n    label_files = glob(os.path.join(labels_dir, 'labeler_*.json'))\n    # También incluimos la auto-etiquetación si existe (ej. self_labeling.json)\n    self_label_file = os.path.join(labels_dir, 'self_labeling.json')\n    if os.path.exists(self_label_file):\n        label_files.append(self_label_file)\n\n    print(f\"Archivos de etiquetas encontrados: {len(label_files)}\")\n\n    for filepath in label_files:\n        labeler_id = os.path.basename(filepath).split('.')[0]\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n                df_temp = pd.DataFrame(data)\n                df_temp['labeler_id'] = labeler_id\n                all_labels.append(df_temp)\n        except Exception as e:\n            print(f\"Error al cargar {filepath}: {e}\")\n\n    if not all_labels:\n        print(\"No se cargaron archivos de etiquetas.\")\n        return pd.DataFrame()\n\n    df_raw = pd.concat(all_labels, ignore_index=True)\n\n    # Convertir a numérico y manejar errores\n    df_raw['attention'] = pd.to_numeric(df_raw['attention'], errors='coerce')\n    df_raw['emotion'] = pd.to_numeric(df_raw['emotion'], errors='coerce')\n\n    # Calcular el consenso: se utiliza la moda (el valor más votado)\n    consensus_df = df_raw.groupby('datetime').agg(\n        attention_consensus=('attention', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n        emotion_consensus=('emotion', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n        n_labels_attention=('attention', 'count'),\n        n_labels_emotion=('emotion', 'count')\n    ).reset_index()\n\n    # Eliminar NaNs generados por filas vacías o datos inválidos\n    consensus_df.dropna(subset=[TARGET_ATTENTION_COL, TARGET_EMOTION_COL], how='all', inplace=True)\n\n    print(f\"Etiquetas de consenso generadas para {len(consensus_df)} momentos.\")\n    return consensus_df\n\n# %% [section]\n# =============================================================================\n# 3. Funciones de Carga y Preprocesamiento de Metadata\n# =============================================================================\n\ndef load_and_flatten_metadata(metadata_dir):\n    \"\"\"\n    Carga todos los archivos JSON de la carpeta 'metadata', extrae las\n    características de bajo nivel y las consolida en un DataFrame.\n    \"\"\"\n    metadata_list = []\n    metadata_files = glob(os.path.join(metadata_dir, '*.json'))\n\n    print(f\"Archivos de metadata encontrados: {len(metadata_files)}\")\n\n    for filepath in tqdm(metadata_files, desc=\"Procesando metadata\"):\n        filename = os.path.basename(filepath)\n        datetime_id = filename.split('.')[0] # Asumiendo que el nombre es la marca de tiempo\n\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n                row = {'datetime': datetime_id, 'metadata_file': filename}\n\n                # 1. Extracción de Bounding Box y Características Demográficas\n                face_data = data.get('person', {}).get('face', {})\n                if not face_data:\n                    continue # Saltar si no hay detección de rostro\n                \n                # Bounding Box\n                bbox = face_data.get('bounding_box', {})\n                row.update({f'bbox_{k}': v for k, v in bbox.items()})\n\n                # Demográficos\n                row['age'] = face_data.get('age')\n                row['gender_name'] = face_data.get('gender', {}).get('gender_name')\n\n                # 2. Extracción de Probabilidades de Emoción (Multimodal - Facial)\n                emotion_probs = face_data.get('emotion', {}).get('probability_emotion', {})\n                row.update({f'prob_emotion_{k}': v for k, v in emotion_probs.items()})\n                row['dominant_emotion'] = face_data.get('emotion', {}).get('dominant_emotion')\n                \n                # 3. Extracción de Features de Postura (Multimodal - Postural)\n                # Se pueden extraer características resumidas de los 'landmarks'\n                # Por ejemplo, la visibilidad promedio de los landmarks del cuerpo\n                landmarks = data.get('person', {}).get('landmarks', [])\n                visibility = [l.get('visibility', 0) for l in landmarks]\n                presence = [l.get('presence', 0) for l in landmarks]\n                \n                row['avg_landmark_visibility'] = np.mean(visibility) if visibility else 0\n                row['avg_landmark_presence'] = np.mean(presence) if presence else 0\n\n                metadata_list.append(row)\n\n        except Exception as e:\n            print(f\"Error al procesar metadata en {filename}: {e}\")\n\n    df_metadata = pd.DataFrame(metadata_list)\n    print(f\"Metadata consolidada para {len(df_metadata)} registros.\")\n    return df_metadata\n\n# %% [section]\n# =============================================================================\n# 4. Función de Preprocesamiento de Imágenes (Recorte Facial)\n# =============================================================================\n\ndef crop_and_save_face(row, output_dir='processed_faces', margin_ratio=0.3):\n    \"\"\"\n    Carga la imagen, utiliza el Bounding Box de la metadata y recorta el rostro,\n    guardando el resultado en una nueva carpeta.\n    \"\"\"\n    img_filename = f\"{row['datetime']}.jpg\"\n    img_path = os.path.join(IMAGES_DIR, img_filename)\n    output_path = os.path.join(output_dir, img_filename)\n\n    # Crear directorio de salida si no existe\n    os.makedirs(output_dir, exist_ok=True)\n\n    if not os.path.exists(img_path):\n        # print(f\"Imagen no encontrada: {img_path}\")\n        return None # Devuelve None si la imagen no existe\n\n    try:\n        # Cargar imagen\n        image = cv2.imread(img_path)\n        if image is None:\n            raise FileNotFoundError(\"Error al cargar la imagen con OpenCV\")\n\n        H, W = image.shape[:2]\n\n        # Extraer coordenadas del Bounding Box\n        x0 = int(row['bbox_x0'])\n        y0 = int(row['bbox_y0'])\n        x1 = int(row['bbox_x1'])\n        y1 = int(row['bbox_y1'])\n\n        # Calcular el ancho y alto originales del BBox\n        w_orig = x1 - x0\n        h_orig = y1 - y0\n\n        # Añadir un margen (30% del tamaño original) para contexto\n        margin_w = int(w_orig * margin_ratio)\n        margin_h = int(h_orig * margin_ratio)\n\n        # Recalcular las coordenadas con margen, asegurando que no se salgan de la imagen\n        x_min = max(0, x0 - margin_w)\n        y_min = max(0, y0 - margin_h)\n        x_max = min(W, x1 + margin_w)\n        y_max = min(H, y1 + margin_h)\n\n        # Recortar la región del rostro con margen\n        cropped_face = image[y_min:y_max, x_min:x_max]\n\n        # Guardar la imagen recortada\n        cv2.imwrite(output_path, cropped_face)\n\n        return output_path\n\n    except Exception as e:\n        print(f\"Error procesando imagen {img_filename}: {e}\")\n        return None\n\ndef visualize_sample_image(df, n_samples=3, processed_dir='processed_faces'):\n    \"\"\"\n    Muestra una comparación visual de la imagen original vs. la imagen recortada.\n    \"\"\"\n    if df.empty:\n        print(\"DataFrame vacío, no se puede visualizar.\")\n        return\n    \n    sample = df.sample(min(n_samples, len(df)))\n    \n    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 5 * n_samples))\n    \n    if n_samples == 1: # Para manejar el caso de una sola muestra\n        axes = np.expand_dims(axes, axis=0)\n\n    for i, (_, row) in enumerate(sample.iterrows()):\n        datetime_id = row['datetime']\n        original_img_path = os.path.join(IMAGES_DIR, f\"{datetime_id}.jpg\")\n        processed_img_path = os.path.join(processed_dir, f\"{datetime_id}.jpg\")\n        \n        # Imagen Original\n        if os.path.exists(original_img_path):\n            img_orig = cv2.cvtColor(cv2.imread(original_img_path), cv2.COLOR_BGR2RGB)\n            axes[i, 0].imshow(img_orig)\n            axes[i, 0].set_title(f\"Original: {datetime_id}\")\n            axes[i, 0].axis('off')\n        \n        # Imagen Recortada\n        if os.path.exists(processed_img_path):\n            img_proc = cv2.cvtColor(cv2.imread(processed_img_path), cv2.COLOR_BGR2RGB)\n            axes[i, 1].imshow(img_proc)\n            attention = ATENTION_MAPPING.get(str(int(row[TARGET_ATTENTION_COL])), 'N/A')\n            emotion = EMOTION_MAPPING.get(str(int(row[TARGET_EMOTION_COL])), 'N/A')\n            axes[i, 1].set_title(f\"Recorte (GT: Att={attention}, Emo={emotion})\")\n            axes[i, 1].axis('off')\n            \n    plt.tight_layout()\n    plt.show()\n\n# %% [execution]\n# =============================================================================\n# 5. Pipeline Principal de Ejecución\n# =============================================================================\nif __name__ == \"__main__\":\n    print(\"--- 1. Carga y Consenso de Etiquetas Humanas (Ground Truth) ---\")\n    df_labels = load_and_merge_labels(LABELS_DIR)\n\n    if df_labels.empty:\n        print(\"Proceso terminado: No se pudo generar el DataFrame de etiquetas.\")\n        exit()\n\n    print(\"\\n--- 2. Carga y Consolidación de Metadata (Características) ---\")\n    df_metadata = load_and_flatten_metadata(METADATA_DIR)\n\n    if df_metadata.empty:\n        print(\"Proceso terminado: No se pudo generar el DataFrame de metadata.\")\n        exit()\n\n    print(\"\\n--- 3. Fusión de Datos (Labels + Metadata) ---\")\n    # Realizar el merge por el identificador de tiempo 'datetime'\n    df_final = pd.merge(df_metadata, df_labels, on='datetime', how='inner')\n\n    print(f\"DataFrame Final Unificado (Registros listos para entrenamiento): {len(df_final)}\")\n    print(df_final.head())\n\n    # --- 4. Preprocesamiento de Imágenes ---\n    PROCESSED_FACES_DIR = os.path.join(BASE_DIR, 'processed_faces')\n    print(f\"\\n--- 4. Procesamiento Visual (Recorte Facial) en: {PROCESSED_FACES_DIR} ---\")\n    \n    # Aplicar la función de recorte a cada fila del DataFrame final\n    tqdm.pandas(desc=\"Recortando y guardando rostros\")\n    df_final['processed_img_path'] = df_final.progress_apply(crop_and_save_face, axis=1, output_dir=PROCESSED_FACES_DIR)\n\n    # Limpiar registros donde la imagen no pudo ser procesada o no existe\n    df_final.dropna(subset=['processed_img_path'], inplace=True)\n    \n    print(f\"\\nDataFrame Final después del procesamiento de imágenes: {len(df_final)}\")\n    \n    # --- 5. Exportación del DataFrame Final ---\n    FINAL_CSV_PATH = os.path.join(BASE_DIR, 'processed_multimodal_data.csv')\n    df_final.to_csv(FINAL_CSV_PATH, index=False)\n    print(f\"\\nDatos consolidados exportados a: {FINAL_CSV_PATH}\")\n\n    # --- 6. Visualización de Muestra ---\n    print(\"\\n--- 6. Visualización de Muestras (Original vs. Recortada) ---\")\n    # Requiere tener las librerías cv2 y matplotlib instaladas.\n    # visualize_sample_image(df_final, n_samples=3, processed_dir=PROCESSED_FACES_DIR)\n\n    print(\"\\nProceso de preprocesamiento visual completado.\")\n# %% [end]","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}